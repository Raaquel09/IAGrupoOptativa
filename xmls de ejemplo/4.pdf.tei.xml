<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-02-26">26 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Babak</forename><surname>Ehteshami Bejnordi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amelie</forename><surname>Royer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
						</author>
						<title level="a" type="main">InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-26">26 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">B2F33A8D63CBCC9C8C68AF188B218C6A</idno>
					<idno type="arXiv">arXiv:2402.16848v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-06T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Jointly learning multiple tasks with a unified model can improve accuracy and data efficiency, but it faces the challenge of task interference, where optimizing one task objective may inadvertently compromise the performance of another. A solution to mitigate this issue is to allocate taskspecific parameters, free from interference, on top of shared features. However, manually designing such architectures is cumbersome, as practitioners need to balance between the overall performance across all tasks and the higher computational cost induced by the newly added parameters. In this work, we propose InterroGate, a novel MTL architecture designed to mitigate task interference while optimizing inference computational efficiency. We employ a learnable gating mechanism to automatically balance the shared and task-specific representations while preserving the performance of all tasks. Crucially, the patterns of parameter sharing and specialization dynamically learned during training, become fixed at inference, resulting in a static, optimized MTL architecture. Through extensive empirical evaluations, we demonstrate SoTA results on three MTL benchmarks using convolutional as well as transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-task learning (MTL) involves learning multiple tasks concurrently with a unified architecture. By leveraging the shared information among related tasks, MTL has the po-tential to improve accuracy and data efficiency. In addition, learning a joint representation reduces the computational and memory costs of the model at inference as visual features relevant to all tasks are extracted only once: This is crucial for many real-life applications where a single device is expected to solve multiple tasks simultaneously under strict compute constraints (e.g. mobile phones, extended reality, self-driving cars, etc.). Despite these potential benefits, in practice, MTL is often met with a key challenge known as negative transfer or task interference <ref type="bibr" target="#b46">(Zhao et al., 2018)</ref>, which refers to the phenomenon where the learning of one task negatively impacts the learning of another task during joint training. While characterizing and solving task interference is an open issue <ref type="bibr" target="#b41">(Wang et al., 2019;</ref><ref type="bibr" target="#b31">Royer et al., 2023)</ref>, there exist two major lines of work to mitigate this problem: (i) Multi-task Optimization (MTO) techniques aim to balance the training process of each task, while (ii) architectural designs carefully allocate shared and task-specific parameters to reduce interference. Specifically, MTO approaches balance the losses/gradients of each task to mitigate the extent of gradient conflicts while optimizing the shared features. However, the results may still be compromised when the tasks rely on inherently different visual cues, making sharing parameters difficult. For instance, text detection and face recognition require learning very different texture information and object scales. An alternative and orthogonal research direction is to allocate additional task-specific parameters, on top of shared generic features, to bypass task interference. In particular, several state-of-the-art methods have proposed task-dependent selection and adaptation of shared features <ref type="bibr" target="#b14">(Guo et al., 2020;</ref><ref type="bibr" target="#b36">Sun et al., 2020;</ref><ref type="bibr" target="#b39">Wallingford et al., 2022;</ref><ref type="bibr" target="#b29">Rahimian et al., 2023)</ref>. However, the dynamic allocation of task-specific features is usually performed one task at a time, and solving all tasks still requires multiple forward passes. Alternatively, Mixture of Experts (MoE) have also been employed to reduce the computational cost of MTL by dynamically routing inputs to a subset of experts <ref type="bibr" target="#b24">(Ma et al., 2018;</ref><ref type="bibr" target="#b15">Hazimeh et al., 2021;</ref><ref type="bibr" target="#b11">Fan et al., 2022;</ref><ref type="bibr" target="#b9">Chen et al., 2023)</ref>. However, the input-dependent routing of MoE is typically hard to efficiently deploy at inference, particularly with batched execution <ref type="bibr" target="#b32">(Sarkar et al., 2023;</ref><ref type="bibr" target="#b43">Yi et al., 2023)</ref>.</p><p>In contrast to previous dynamic architectures, we learn to balance shared and task-specific features jointly for all tasks, which allows us to predict all task outputs in a single forward pass. In addition, we propose to regulate the expected inference computational cost through a budget-aware regularization during training. By doing so, we aim to depart from a common trend in MTL that heavily focuses on accuracy while neglecting computational efficiency <ref type="bibr" target="#b27">(Misra et al., 2016;</ref><ref type="bibr" target="#b37">Vandenhende et al., 2020)</ref>.</p><p>In this paper, we introduce InterroGate, a novel MTL architecture to mitigate task interference while optimizing computational efficiency during inference. Our method learns the per-layer optimal balance between sharing and specializing representations for a desired computational budget. In particular, we leverage a shared network branch which is used as a general communication channel through which the task-specific branches interact with each other. This communication is enabled through a novel gating mechanism which learns for each task and layer to select parameters from either the shared branch or their task-specific branch. To enhance the learning of the gating behaviour, we harness single task baseline weights to initialize task-specific branches.</p><p>InterroGate primarily aims to optimize efficiency in the inference phase, crucial in real-world applications. While the gate dynamically learns to select between a large pool of task-specific and shared parameters during training, at inference, the learned gating patterns are static and thus can be used to prune the unselected parameters in the shared and task-specific branches: As a result, InterroGate collapses to a simpler, highly efficient, static architecture at inference time, suitable for batch processing. We control the trade-off between inference computational cost and multi-task performance, by regularizing the gates using a sparsity objective. In summary, our contributions are as follows:</p><p>• We propose a novel multi-task learning framework that learns the optimal parameter sharing and specialization patterns for all tasks, in tandem with the model parameters, enhancing multi-task learning efficiency and effectiveness.</p><p>• We enable a training mechanism to control the trade-off between multi-task performance and inference compute cost. Our proposed gating mechanism finds the optimal balance between selecting shared and specialized parameters for each task, within a desired computational budget, controlled with a sparsity objective. This, subsequently, enables a simple process for creating a range of models on the efficiency/accuracy trade-off spectrum, as opposed to most other MTL methods.</p><p>• Our proposed method is designed to optimize inference-time efficiency. Post-training, the unselected parameters by the gates are pruned from the model, resulting in a simpler, highly efficient neural network.</p><p>In addition, our feature fusion strategy allows to predict all tasks in a single forward pass, critical in many real-world applications.</p><p>• Through extensive empirical evaluations, we report SoTA results consistently on three multi-tasking benchmarks with various convolutional and transformerbased backbones. We then further investigate the proposed framework through ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-task Optimization (MTO) methods aim to automatically balance the different tasks when optimizing shared parameters to maximize average performance. Loss-based methods <ref type="bibr" target="#b18">(Kendall et al., 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2022)</ref> are usually scalable and adaptively scale task losses based on certain statistics (e.g. task output variance); Gradient-based methods <ref type="bibr" target="#b33">(Sener &amp; Koltun, 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2018b;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b20">Liu et al., 2021;</ref><ref type="bibr">Javaloy &amp; Valera, 2021)</ref> are more costly in practice as they require storing a gradient per task, but usually yield higher performance. Orthogonal to these optimization methods, several research directions investigate how to design architectures that inherently mitigate task-interference, as described below.</p><p>Task Grouping approaches investigate which groups of tasks can safely share encoder parameters without task interference. For instance <ref type="bibr" target="#b35">(Standley et al., 2020;</ref><ref type="bibr" target="#b12">Fifty et al., 2021)</ref> identify "task affinities" as a guide to isolate parameters of tasks most likely to interfere with one another. Similarly, <ref type="bibr" target="#b14">(Guo et al., 2020)</ref> apply neural architecture search techniques to design MTL architectures. However, exploring these large architecture search spaces is a costly process.</p><p>Hard Parameter Sharing works such as Cross-Stitch <ref type="bibr" target="#b27">(Misra et al., 2016)</ref>, MTAN <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> or MuIT <ref type="bibr" target="#b2">(Bhattacharjee et al., 2022)</ref> propose to learn the task parameter sharing design alongside the model features. However, most of these works mainly focus on improving the accuracy of the model while neglecting the computational cost: For instance, <ref type="bibr" target="#b27">(Misra et al., 2016;</ref><ref type="bibr" target="#b13">Gao et al., 2020)</ref> require a full network per task and improve MTL performance through lightweight adapters across task branches. <ref type="bibr" target="#b21">(Liu et al., 2019;</ref><ref type="bibr" target="#b2">Bhattacharjee et al., 2022)</ref> use task-specific attention module on top of a shared feature encoder, but the cost of the task-specific decoder heads often dominates the final architecture.</p><p>Conditional Compute approaches learn task-specific gating of model parameters: For instance <ref type="bibr" target="#b36">(Sun et al., 2020;</ref><ref type="bibr" target="#b39">Wallingford et al., 2022)</ref> learn to select a subset of the most relevant layers when adapting the network to a new downstream task. Piggyback <ref type="bibr" target="#b25">(Mallya et al., 2018)</ref>  To decide between shared ψ ℓ or task-specific φ ℓ t features, each task relies on its own gating module G ℓ t . The resulting channel-mixed feature-map φ ′ℓ t is then fed to the next task-specific layer. The input to the shared branch for the next layer is constructed by linearly combining the task-specific features of all tasks using the learned parameter β ℓ t . During inference, the parameters (shared or task-specific) that are not chosen by the gates are removed from the model, resulting in a plain neural network architecture.</p><p>to multiple tasks by learning a set of per-task sparse masks for the network parameters. Similarly, <ref type="bibr" target="#b1">(Berriel et al., 2019)</ref> select the most relevant feature channels using learnable gates. Finally, Mixture of Experts (MoE) <ref type="bibr" target="#b24">(Ma et al., 2018;</ref><ref type="bibr" target="#b15">Hazimeh et al., 2021;</ref><ref type="bibr" target="#b11">Fan et al., 2022;</ref><ref type="bibr" target="#b9">Chen et al., 2023)</ref> leverage sparse gating to select a subset of experts for each input example.</p><p>Nevertheless, due to the dynamic nature of these works at inference, obtaining all task predictions is computationally inefficient as it often requires one forward pass through the model per task. Therefore, these methods are less suited for MTL settings requiring solving all tasks concurrently. Additionally, dynamic expert selection in MoEs requires either storing all expert weights on-chip, increasing memory demands, or frequent off-chip data transfers to load necessary experts, leading to significant overheads, complicating their efficient inference on resource-constrained devices <ref type="bibr" target="#b32">(Sarkar et al., 2023;</ref><ref type="bibr" target="#b43">Yi et al., 2023)</ref>. In contrast, InterroGate focuses on optimizing efficiency and is capable of addressing all tasks simultaneously, aligning with many practical real-world needs. InterroGate employs learned gating patterns to prune unselected parameters, resulting in a more streamlined and efficient static architecture during inference, well-suited for batch processing. Finally, closest to our work is <ref type="bibr" target="#b3">(Bragman et al., 2019)</ref>, which proposes a probabilistic allocation of convolutional filters as task-specific or shared. However, this design only allows for the shared features to send information to the task-specific branches. In contrast, our gating mechanism allows for information to flow in any direction between the shared and task-specific features, thereby enabling cross-task transfer in every layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">InterroGate</head><p>Given T tasks, we aim to learn a flexible allocation of shared and task-specific parameters, while optimizing the trade-off between accuracy and efficiency. Specifically, an Interro-Gate model is characterized by task-specific parameters {Φ t } T t=1 and shared parameters Ψ. In addition, discrete gates (with parameters α) are trained to only select a subset of the most relevant features in both the shared and task-specific branches, thereby reducing the model's computational cost. Under this formalism, the model and gate parameters are trained end-to-end by minimizing the classical MTL objective:</p><formula xml:id="formula_0">L({Φ t } T t=1 , Ψ, α) = T t=1 ω t L t (X, Y t ; Φ t , Ψ, α),<label>(1)</label></formula><p>where X and Y t are the input data and corresponding labels for task t, L t represents the loss function associated to task t, and ω t are hyperparameter coefficients which allow for balancing the importance of each task in the overall objective. In the rest of the section, we describe how we learn and implement the feature-level routing mechanism characterized by α. We focus on convolutional architectures in Section 3.1, and discuss the case of transformer-based models in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning to Share, Specialize and Prune</head><p>Figure <ref type="figure">1</ref> presents an overview of the proposed InterroGate framework. Formally, let</p><formula xml:id="formula_1">ψ ℓ ∈ R C ℓ ×W ℓ ×H ℓ and φ ℓ t ∈ R C ℓ ×W ℓ ×H ℓ</formula><p>represent the shared and task-specific features at layer ℓ of our multi-task network, respectively. In each layer ℓ, the gating module G ℓ t of task t selects relevant channels from either ψ ℓ and φ ℓ t . The output of this hard routing operation yields features φ ′ℓ t :</p><formula xml:id="formula_2">φ ′ℓ t = G ℓ t (α ℓ t ) ⊙ φ ℓ t + (1 − G ℓ t (α ℓ t )) ⊙ ψ ℓ , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ⊙ is the Hadamard product and α ℓ t ∈ R C ℓ denotes the learnable gate parameters for task t at layer ℓ and the gating module G ℓ t outputs a vector in {0, 1} C ℓ , encoding the binary selection for each channel. These intermediate features are then fed to the next task-specific layer to form the features φ ℓ+1 t = F (φ ′ℓ t ; Φ ℓ t ). Similarly, we construct the shared features of the next layer ℓ + 1 by mixing the previous layer's task-specific feature maps. However, how to best combine these T feature maps is a harder problem than the pairwise selection described in (2). Therefore, we let the model learn its own soft combination weights and the resulting mixing operation for the shared features is defined as follows:</p><formula xml:id="formula_4">ψ ′ℓ = T t=1 softmax t=1...T (β ℓ t ) ⊙ φ ′ℓ t ,<label>(3)</label></formula><p>where β ℓ ∈ R C ℓ ×T denotes the learnable parameters used to linearly combine the task-specific features and form the shared feature map of the next layer. Similar to the task-specific branch, these intermediate features are then fed to a convolutional block to form the features ψ ℓ+1 = F (ψ ′ℓ ; Ψ ℓ ). Finally, note that there is no direct information flow between the shared features of one layer to the next, i.e., ψ ℓ and ψ ℓ+1 : Intuitively, the shared feature branch can be interpreted as a general communication channel which the task-specific branches communicate with one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementing the Discrete Routing Mechanism</head><p>During training, the model features and gates are trained jointly and end-to-end. In (2), the gating modules G ℓ t each output a binary vector over channels in {0, 1} C , where 0 means choosing the shared feature at this channel index, while 1 means choosing the specialized feature for the respective task t. In practice, we implement G as a sigmoid operation applied to the learnable parameter α, followed by a thresholding operation at 0.5. Due to the non-differentiable nature of this operation, we adopt the straight-through estimation (STE) during training <ref type="bibr" target="#b0">(Bengio et al., 2013</ref>): In the backward pass, STE approximates the gradient flowing through the thresholding operation as the identity function.</p><p>At inference, since the gate modules do not depend on the input data, our proposed InterroGate method converts to a static neural network architecture, where feature maps are pruned following the learned gating patterns: To be more specific, for a given layer ℓ and task t, we first collect all channels for which the gate G ℓ t (α ℓ t ) outputs 0; Then, we simply prune the corresponding task-specific weights in Φ ℓ−1 t . Similarly, we can prune away weights from the shared branch Ψ ℓ−1 if the corresponding channels are never chosen by any of the tasks in the mixing operation of (2). The pseudo-code for the complete unified encoder forwardpass is detailed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sparsity Regularization</head><p>During training, we additionally control the proportion of shared versus task-specific features usage by regularizing the gating module G. This allows us to reduce the computational cost, as more of the task-specific weights can be pruned away at inference. We implement the regularizer term as a hinge loss over the gating activations for taskspecific features:</p><formula xml:id="formula_5">L sparsity (α) = 1 T T t=1 max 0, 1 L L ℓ=1 σ(α ℓ t ) − τ t ,<label>(4)</label></formula><p>where σ is the sigmoid function and τ t is a task-specific hinge target. The parameter τ allows to control the proportion of active gates at each specific layer by setting a soft upper limit for active task-specific parameters. A lower hinge target value encourages more sharing of features while a higher value gives more flexibility to select task-specific features albeit at the cost of higher computational cost.</p><p>Our final training objective is a combination of the multitask objective and sparsity regularizer:</p><formula xml:id="formula_6">L = L({Φ t } T t=1 , Ψ, α, β) + λ s L sparsity (α),<label>(5)</label></formula><p>where λ s is a hyperparameter balancing the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets and Backbones. We evaluate the performance of InterroGate on three popular datasets: CelebA <ref type="bibr" target="#b23">(Liu et al., 2015)</ref>, NYUD-v2 <ref type="bibr" target="#b34">(Silberman et al., 2012)</ref>, and PASCAL-Context <ref type="bibr" target="#b6">(Chen et al., 2014)</ref>. CelebA is a large-scale face attributes dataset, consisting of more than 200k celebrity images, each labeled with 40 attribute annotations. We consider the age, gender, and clothes attributes to form three output classification tasks for our MTL setup and use binary cross-entropy to train the model. The NYUD-v2 dataset is designed for semantic segmentation and depth estimation tasks. It comprises 795 train and 654 test images taken from various indoor scenes in New York City, with pixel-wise annotation for semantic segmentation and depth estimation. Following recent work <ref type="bibr" target="#b42">(Xu et al., 2018;</ref><ref type="bibr" target="#b45">Zhang et al., 2019;</ref><ref type="bibr" target="#b26">Maninis et al., 2019)</ref>, we also incorporate the surface normal prediction task, obtaining annotations directly from the depth ground truth. We use the mean intersection over union (mIoU) and root mean square error (rmse) to evaluate the semantic segmentation and depth estimation tasks, respectively. We use the mean error (mErr) in the predicted angles to evaluate the surface normals. Finally, the PASCAL-Context dataset is an extension of the PASCAL VOC 2010 dataset <ref type="bibr" target="#b10">(Everingham et al., 2010</ref>) and provides a comprehensive scene understanding benchmark by labeling images for semantic segmentation, human parts segmentation, semantic edge detection, surface normal estimation, and saliency detection. The dataset consists of 4,998 train images and 5,105 test images. The semantic segmentation, saliency estimation, and human part segmentation tasks are evaluated using mIoU. Similar to NYUD, mErr is used to evaluate the surface normal predictions.</p><p>We use ResNet-20 <ref type="bibr" target="#b16">(He et al., 2016)</ref> as the backbone in our CelebA experiments, with simple linear heads for the task-specific predictions. For the NYUD-v2 dataset, we use ResNet-50 with dilated convolutions and HRNet-18 following <ref type="bibr" target="#b38">(Vandenhende et al., 2021)</ref>. We also present results using a dense prediction transformer (DPT) <ref type="bibr" target="#b30">(Ranftl et al., 2021)</ref>, with a ViT-base and -small backbone. Finally, on PASCAL-Context, we use a ResNet-18 backbone. For both NYUD and PASCAL, we use dense prediction heads to output the task predictions, as described in Appendix A.</p><p>SoTA Baselines and Metrics. To establish upper and lower bounds of MTL performance, we always compare our models to the Single-Task baseline (STL), which is the performance obtained when training an independent network for each task, as well as the uniform MTL baseline where the model's encoder (backbone) is shared by all tasks.</p><p>In addition, we compare InterroGate to encoder-based methods including Cross-stitch <ref type="bibr" target="#b27">(Misra et al., 2016)</ref> and MTAN <ref type="bibr" target="#b21">(Liu et al., 2019)</ref>, as well as MTO approaches such as uncertainty weighting <ref type="bibr" target="#b18">(Kendall et al., 2018)</ref>, DWA <ref type="bibr" target="#b21">(Liu et al., 2019)</ref>, and Auto-λ <ref type="bibr" target="#b22">(Liu et al., 2022)</ref>, PCGrad <ref type="bibr" target="#b44">(Yu et al., 2020)</ref>, CAGrad <ref type="bibr" target="#b20">(Liu et al., 2021)</ref>, MGDA-UB <ref type="bibr" target="#b33">(Sener &amp; Koltun, 2018)</ref>, and RLW <ref type="bibr" target="#b19">(Lin et al., 2022)</ref>. Following <ref type="bibr" target="#b26">(Maninis et al., 2019)</ref>, our main metric is the multi-task performance ∆ MTL of a model m as the averaged normalized drop in performance w.r.t. the single-task baselines b:</p><formula xml:id="formula_7">∆ MTL = 1 T T i=1 (−1) li (M m,i − M b,i ) /M b,i<label>(6)</label></formula><p>where l i = 1 if a lower value means better performance for metric M i of task i, and 0 otherwise. Furthermore, similar to <ref type="bibr" target="#b28">(Navon et al., 2022)</ref>, we compute the mean rank (MR) as the average rank of each method across the different tasks, where a lower MR indicates better performance. All reported results for InterroGate and baselines are averaged across 3 random seeds.</p><p>Finally, to generate the trade-off curve between MTL performance and compute cost of InterroGate in Figure <ref type="figure" target="#fig_0">2</ref> and all the tables, we sweep over the gate sparsity regularizer weight, λ s , in the range of {1, 3, 5, 7, 10} • 10 −2 . The taskspecific targets τ in (4) also impact the computation cost.</p><p>Intuitively, tasks with significant performance degradation benefit from more task-specific parameters, i.e., from higher values of the hyperparameters τ t . We analyze the gating patterns for sharing and specialization in section 4.3.2 and, through ablation experiments, we further discuss the impact of sparsity targets {τ t } T t=1 in Appendix E. While InterroGate primarily aims at improving the inference cost efficiency, we also measure and report training time comparison between our method and the baselines, on the PASCAL-Context <ref type="bibr" target="#b6">(Chen et al., 2014)</ref> dataset in Appendix F.</p><p>Training Pipeline. For initialization, we use pretrained ImageNet weights for the single-task and multi-task baseline. For InterroGate, the shared branch is initialized with ImageNet weights while the task-specific branches are with their corresponding single-task weights. Finally, we discover that employing a separate optimizer for the gates improves the convergence of the model. We further describe training hyperparameters in Appendix A. All experiments were conducted on a single NVIDIA V100 GPU and we use the same training setup and hyperparameters across all MTL approaches included in our comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>In this section, we present the results of InterroGate, singletask (STL), multi-task baselines, and the competing MTL approaches, on the CelebA, NYUD-v2, and PASCAL-Context datasets. In Section 4.3, we will also present several ablation studies on (i) how we design the sparsity regularization loss, (ii) on the qualitative sharing/specialization patterns learned by the gate, and finally, (ii) on the impact of the backbone model capacity on MTL performance. As mentioned earlier, we also present an ablation on the impact of the sparsity targets {τ t } T t=1 in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">CELEBA</head><p>Figure <ref type="figure" target="#fig_0">2a</ref> shows the trade-off between MTL performance and the computational cost (FLOPs) for InterroGate, MTL uniform, and STL baselines on the CelebA dataset, for 3 different widths for the ResNet-20 backbone: quarter, half, and original capacity. We report the detailed results in Ta-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">NYUD-V2</head><p>Table <ref type="table" target="#tab_2">1</ref> and 2 present the results on the NYUD-v2 dataset, using the HRNet-18 and ResNet-50 backbones, respectively. Figure <ref type="figure" target="#fig_0">2b</ref> illustrates the trade-off between ∆ MTL and computational cost of various methods using the HRNet-18 backbone. We additionally report the parameter count and the standard deviation of the ∆ MTL scores in Tables <ref type="table" target="#tab_2">9 and  10</ref>, in Appendix D. As can be seen, most MTL methods improve the accuracy on the segmentation and depth estimation tasks, while surface normal prediction significantly drops.</p><p>While MTL uniform and MTO strategies operate at the lowest computational cost by sharing the full backbone, they fail to compensate for this drop in performance. MTO approaches often show mixed results across the two backbones. While CAGrad <ref type="bibr" target="#b20">(Liu et al., 2021)</ref> and uncertainty weighting <ref type="bibr" target="#b18">(Kendall et al., 2018)</ref>  Table <ref type="table" target="#tab_4">3</ref> and 4 report the performance of DPT trained models with the ViT-base and ViT-small backbones. The MTL uniform and MTO baselines, display reduced computational cost, yet once again manifesting a performance drop in the normals prediction task. Similar to the trend between HRNet-18 and ResNet-50, the performance drop is more substantial for the smaller model, ViT-small, indicating that task interference is more prominent in small capacity settings. In comparison, InterroGate consistently demonstrates a more favorable balance between the computational cost and the overall MTL accuracy across varied backbones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">PASCAL-CONTEXT</head><p>Table <ref type="table" target="#tab_6">5</ref> summarizes the results of our experiments on the PASCAL-context dataset encompassing five tasks. Note that following previous work, we use the task losses' weights ω t from <ref type="bibr" target="#b26">(Maninis et al., 2019)</ref> for all MTL methods, but also report MTL uniform results as a reference. Figure <ref type="figure" target="#fig_0">2c</ref> illustrates the trade-off between ∆ MTL and the computational cost of all models. The STL baseline outperforms most methods on the semantic segmentation and normals prediction tasks with a score of 14.70 and 66.1, while incurring a computational cost of 670 GFlops. Among the baseline MTL and MTO approaches, there is a notable degradation in surface normal prediction. Finally, as witnessed in prior works <ref type="bibr" target="#b26">(Maninis et al., 2019;</ref><ref type="bibr" target="#b37">Vandenhende et al., 2020;</ref><ref type="bibr" target="#b4">Brüggemann et al., 2021)</ref>, we observe that most MTL and MTO baselines struggle to reach STL performance. Among competing methods, MTAN and MGDA-UB yield the best MTL performance versus computational cost tradeoff, however, both suffer from a notable decline in normals prediction performance.</p><p>At its highest compute budget (no sparsity loss and negligible computational savings), InterroGate outperforms the STL baseline, notably in Saliency and Human parts prediction tasks, and achieves an overall ∆ MTL of +0.56. As we reduce the computational cost by increasing the sparsity loss weight λ s , we observe a graceful decline in the multi-task performance that outperforms competing methods. Our In-terroGate models additionally obtain more favorable MR scores compared to the baselines. This emphasizes our model's ability to maintain a favorable balance between compute cost and multi-task performance across computational budgets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">SPARSITY LOSS</head><p>To study the effect of the sparsity loss defined in Equation <ref type="formula" target="#formula_5">4</ref>, we conduct the following two experiments: First, we omit the sparsity regularization loss (λ s = 0): As can be seen in the first row of Table <ref type="table" target="#tab_7">6</ref>, InterroGate outperforms the single task baseline, but the computational savings are very limited.</p><p>In the second ablation experiment, we compare the use of the L 1 hinge loss with a standard L 1 loss function as the sparsity regularizer: The results of Table <ref type="table" target="#tab_7">6</ref> show that the hinge loss formulation consistently yields better trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">LEARNED SHARING AND SPECIALIZATION PATTERNS</head><p>We then investigate the gating patterns that the model converges to. Specifically, we want to observe how much each task contributes to and benefits from the shared representations. To that aim, we monitor (i) the percentage of taskspecific representations selected by each task (captured by the gates G t (α t )), as well as (ii) how much the features specific to each task contribute to the formation of the shared feature bank (captured by the learned combination weights β); We visualize these values for the five tasks of the Pascal-Context dataset in Figure <ref type="figure" target="#fig_1">3</ref> for different sparsity regularizers: with hinge loss (left), with L 1 loss at medium (middle) and high pruning levels (right). We also present these results for all layers in Figure <ref type="figure" target="#fig_3">4</ref>, Appendix D.</p><p>In all settings, the semantic segmentation task makes the largest contribution to the shared branch, followed by the normals prediction tasks. It is worth noting that the amount of feature contribution to the shared branch can also be largely influenced by other tasks' loss functions. In this situation, we observe that if the normal task lacks enough task-specific features (as seen in the middle and right models), its performance deteriorates significantly. In contrast, when it acquires sufficient task-specific features, it maintains a high accuracy (left). Intriguingly, the features of the normal task become less interesting to other tasks in this scenario possibly due to increased specialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">IMPACT OF MODEL CAPACITY</head><p>In this section, we conduct an ablation study to analyze the relationship between model capacity and multi-task performance. We progressively reduce the width of ResNet-50 and ResNet-20 to half and a quarter of the original sizes for NYUD-v2 and CelebA datasets, respectively. Shrinking the model size, as observed in Table <ref type="table" target="#tab_8">7</ref>, incurs progressively more harmful effect on multi-task performance compared to the single task baseline. In comparison, our proposed InterroGate approach consistently finds a favorable trade-off between capacity and performance and improves over single task performances, across all capacity ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>In this paper, we propose InterroGate, a novel framework to address the fundamental challenges of task interference and computational constraints in MTL. InterroGate leverages a learnable gating mechanism that enables individual tasks to select and combine channels from both a specialized and shared feature set. This framework promotes an asymmetric flow of information, where each task is free to decide how much it contributes to -or takes from -the shared branch. By regularizing the learnable gates, we can strike a balance between task-specific resource allocation and overarching computational costs. InterroGate demonstrates state-of-the-art performance across various architectures and on notable benchmarks such as CelebA, NYUD-v2, and Pascal-Context. The gating mechanism in InterroGate operates over the channel dimension, or the embedding dimension in the case of ViTs, which in its current form does not support structured pruning for attention matrix computations. Future explorations might integrate approaches like patch-token gating to further optimize computational efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p><p>Algorithm 1 Pseudo-code for unified representation encoder Given: Task-specific encoder representations ψ 0 , φ 0 1 , ..., φ 0 T ← x Set initial shared and task-specific features for ℓ = 1 to L do for t = 1 to T do</p><formula xml:id="formula_8">• x ∈ R 3×W ×H Input image • T, L ∈ R</formula><formula xml:id="formula_9">φ ′ℓ t ← G ℓ t (α ℓ t ) ⊙ φ ℓ t + (1 − G ℓ t (α ℓ t )) ⊙ ψ ℓ (2)</formula><p>Choose shared and task-specific features</p><formula xml:id="formula_10">φ ℓ+1 t ← F (φ ′ℓ t ; Φ ℓ t ) Compute task-specific features end for ψ ′ℓ = T t=1 softmax t=1...T (β ℓ t ) ⊙ φ ′ℓ t (3)</formula><p>Combine task-specific features to form shared ones</p><formula xml:id="formula_11">ψ ℓ+1 ← F (ψ ′ℓ ; Ψ ℓ )</formula><p>Compute shared features end for where α ℓ q,t , α ℓ k,t , α ℓ v,t are the learnable gating parameters mixing the task-specific and shared projections for queries, keys and values, respectively. f l q,t , f l k,t , f l v,t are the linear projections for query, key and value for the task t, while f l q , f l k , f l v are the corresponding shared projections. Once the task-specific representations are formed, the shared embeddings for the next block are computed by a learned mixing of the task-specific feature followed by a linear projection, as described in (3). Similarly, we apply this gating mechanism to the final linear projection of the multi-head self-attention, as well as the linear layers in the feed-forward networks in-between each self-attention block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Full Results on CelebA and NYUD-v2</head><p>In Table <ref type="table" target="#tab_10">8</ref>, we report results on the CelebA dataset for different model capacities: Here, InterroGate is compared to the STL and standard MTL methods with different model width: at original, half and quarter of the original model width. In Tables <ref type="table" target="#tab_2">9 and 10</ref>, we report the complete results on NYUD-v2 dataset using HRNet-18 and ResNet-50 backbones, including the parameter count and standard deviation of the ∆ MTL scores.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Sharing/Specialization Patterns</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Accuracy vs. floating-point operations (FLOP) trade-off curves for InterroGate and SoTA MTL methods. (a) Results on CelebA using ResNet-20 backbone at three different widths (Original, Half, and Quarter). (b) NYUD-v2 using HRNet-18 backbone, and (c) ResNet-18 on PASCAL-Context. To avoid clutter, we present the six highest-performing MTL baselines in (b) &amp; (c). The single task baseline in (b) has 65.1 GFLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>tFigure 3 .</head><label>3</label><figDesc>Figure 3. The task-specific representation selection ratio (top) versus proportions of maximum contributions to the shared branch (bottom) for InterroGate with hinge loss (left), L1 loss with medium pruning (middle) and L1 loss with high pruning (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e r 1 0 L a y e r 1 1 L a y e r 1 2 L a y e r 1 3 L a y e r 1 4 L a y e r 1 5 L a y e r 1 6 L a y e r 1 7 L a y e r 1 8 L a y e r uniform L1 loss (high pruning)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Sharing and specialization patterns on pascal context dataset with ResNet-18 backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Sweeping over different {τt} on the NYUD-v2 experiments with HRNet-18 backbone. We plot the MTL performance ∆MT L against the total number of FLOPs, then color each scatter point by the value of τt when the task t is (a) segmentation, (b) depth and (c) normals.</figDesc><graphic coords="16,189.09,372.13,218.69,160.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Sweeping over different {τt} on the PASCAL-Context. We plot the MTL performance ∆MT L against the total number of FLOPs, then color each scatter point by the value of τt when the task t is (a) edges, (b) normals (c) human parts, (d) saliency and (e) segmentation.</figDesc><graphic coords="17,189.09,458.84,218.69,160.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>adapts a pretrained network Overview of the proposed InterroGate framework: The original encoder layers are substituted with InterroGate layers. The input to the layer is t + 1 feature maps, one shared representation and t task-specific representations.</figDesc><table><row><cell>Training</cell><cell></cell></row><row><cell>Shared Branch</cell><cell></cell></row><row><cell cols="2">An InterroGate Layer</cell></row><row><cell>Task Heads</cell><cell></cell></row><row><cell></cell><cell>Fixed</cell></row><row><cell>Encoder</cell><cell>fusion</cell></row><row><cell>Inference</cell><cell></cell></row><row><cell cols="2">InterroGate at Inference</cell></row><row><cell>Figure 1.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on NYUD-v2 using HRNet-18.</figDesc><table><row><cell>are the best performing MTO baselines</cell></row><row><cell>using ResNet-50 and HRNet-18 backbones, respectively,</cell></row><row><cell>they show negligible improvement over uniform MTL when</cell></row><row><cell>applied to the alternate backbones. In contrast, among the</cell></row><row><cell>encoder-based methods, Cross-stitch largely retains perfor-</cell></row><row><cell>mance on normal estimation and achieves a positive ∆ MTL</cell></row><row><cell>score of +1.66. However, this comes at a substantial com-</cell></row><row><cell>putational cost and parameter count, close to that of the</cell></row><row><cell>STL baseline. In comparison, InterroGate achieves an over-</cell></row><row><cell>all ∆</cell></row></table><note>MTL score of +2.06 and +2.04 using HRNet-18 and ResNet-50, respectively, at a lower computational cost. At an equal parameter count of 92.4 M, InterroGate surpasses MTAN using the ResNet-50 backbone, exhibiting a ∆ MTL score of +1.16, in contrast to MTAN's -0.84. MODEL SEMSEG ↑ DEPTH ↓ NORMALS ↓ ∆ MTL (%) ↑ FLOPS (G) MR↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on NYUD-v2 using ResNet-50.</figDesc><table><row><cell>STL</cell><cell>43.20</cell><cell>0.599</cell><cell>19.42</cell><cell>0</cell><cell>1149</cell><cell>9.0</cell></row><row><cell>MTL (UNI.)</cell><cell>43.39</cell><cell>0.586</cell><cell>21.70</cell><cell>-3.04</cell><cell>683</cell><cell>9.7</cell></row><row><cell>DWA</cell><cell>43.60</cell><cell>0.593</cell><cell>21.64</cell><cell>-3.16</cell><cell>683</cell><cell>9.7</cell></row><row><cell>UNCERTAINTY</cell><cell>43.47</cell><cell>0.594</cell><cell>21.42</cell><cell>-2.95</cell><cell>683</cell><cell>10.0</cell></row><row><cell>AUTO-λ</cell><cell>43.57</cell><cell>0.588</cell><cell>21.75</cell><cell>-3.10</cell><cell>683</cell><cell>10.0</cell></row><row><cell>RLW</cell><cell>43.49</cell><cell>0.587</cell><cell>21.54</cell><cell>-2.74</cell><cell>683</cell><cell>8.3</cell></row><row><cell>PCGRAD</cell><cell>43.74</cell><cell>0.588</cell><cell>21.55</cell><cell>-2.66</cell><cell>683</cell><cell>7.3</cell></row><row><cell>CAGRAD</cell><cell>43.57</cell><cell>0.583</cell><cell>21.55</cell><cell>-2.49</cell><cell>683</cell><cell>7.0</cell></row><row><cell>MGDA-UB</cell><cell>42.56</cell><cell>0.586</cell><cell>21.76</cell><cell>-3.83</cell><cell>683</cell><cell>11.3</cell></row><row><cell>MTAN</cell><cell>44.92</cell><cell>0.585</cell><cell>21.14</cell><cell>-0.84</cell><cell>683</cell><cell>4.0</cell></row><row><cell>CROSS-STITCH</cell><cell>44.19</cell><cell>0.577</cell><cell>19.62</cell><cell>+1.66</cell><cell>1151</cell><cell>2.7</cell></row><row><cell>INTERROGATE</cell><cell>44.38</cell><cell>0.576</cell><cell>19.50</cell><cell>+2.04</cell><cell>916</cell><cell>1.7</cell></row><row><cell>INTERROGATE</cell><cell>43.63</cell><cell>0.577</cell><cell>19.66</cell><cell>+1.16</cell><cell>892</cell><cell>3.7</cell></row><row><cell>INTERROGATE</cell><cell>43.05</cell><cell>0.589</cell><cell>19.95</cell><cell>-0.50</cell><cell>794</cell><cell>9.7</cell></row></table><note>MODEL SEMSEG ↑ DEPTH ↓ NORMALS ↓ ∆ MTL (%) ↑ FLOPS (G) MR↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results on NYUD-v2 using DPT with ViT-base.</figDesc><table><row><cell>MODEL</cell><cell cols="6">SEMSEG ↑ DEPTH ↓ NORMALS ↓ ∆ MTL (%) ↑ FLOPS (G) MR↓</cell></row><row><cell>STL</cell><cell>51.65</cell><cell>0.548</cell><cell>19.04</cell><cell>0</cell><cell>759</cell><cell>5.0</cell></row><row><cell>MTL (UNI.)</cell><cell>51.38</cell><cell>0.539</cell><cell>20.73</cell><cell>-2.57</cell><cell>294</cell><cell>7.3</cell></row><row><cell>DWA</cell><cell>51.66</cell><cell>0.536</cell><cell>20.98</cell><cell>-2.66</cell><cell>294</cell><cell>6.0</cell></row><row><cell>UNCERTAINTY</cell><cell>51.87</cell><cell>0.5352</cell><cell>20.72</cell><cell>-2.02</cell><cell>294</cell><cell>4.0</cell></row><row><cell>INTERROGATE</cell><cell>51.98</cell><cell>0.528</cell><cell>19.10</cell><cell>+1.32</cell><cell>626</cell><cell>1.3</cell></row><row><cell>INTERROGATE</cell><cell>51.46</cell><cell>0.536</cell><cell>19.34</cell><cell>+0.08</cell><cell>483</cell><cell>5.0</cell></row><row><cell>INTERROGATE</cell><cell>51.66</cell><cell>0.534</cell><cell>20.16</cell><cell>-1.10</cell><cell>387</cell><cell>3.7</cell></row><row><cell>INTERROGATE</cell><cell>51.71</cell><cell>0.535</cell><cell>20.38</cell><cell>-1.51</cell><cell>324</cell><cell>3.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results on NYUD-v2 using DPT with ViT-small.</figDesc><table><row><cell>STL</cell><cell>46.58</cell><cell>0.583</cell><cell>21.22</cell><cell>0</cell><cell>248</cell><cell>4.0</cell></row><row><cell>MTL (UNI.)</cell><cell>45.32</cell><cell>0.576</cell><cell>22.86</cell><cell>-3.04</cell><cell>118</cell><cell>7.3</cell></row><row><cell>DWA</cell><cell>45.74</cell><cell>0.5721</cell><cell>22.94</cell><cell>-2.68</cell><cell>118</cell><cell>5.7</cell></row><row><cell>UNCERTAINTY</cell><cell>45.67</cell><cell>0.5737</cell><cell>22.80</cell><cell>-2.60</cell><cell>118</cell><cell>5.7</cell></row><row><cell>INTERROGATE</cell><cell>45.96</cell><cell>0.5648</cell><cell>20.77</cell><cell>+1.30</cell><cell>229</cell><cell>1.7</cell></row><row><cell>INTERROGATE</cell><cell>45.34</cell><cell>0.5671</cell><cell>20.96</cell><cell>+0.43</cell><cell>183</cell><cell>4.0</cell></row><row><cell>INTERROGATE</cell><cell>45.57</cell><cell>0.5666</cell><cell>21.36</cell><cell>+0.00</cell><cell>168</cell><cell>4.0</cell></row><row><cell>INTERROGATE</cell><cell>45.99</cell><cell>0.5713</cell><cell>22.02</cell><cell>-1.01</cell><cell>132</cell><cell>3.7</cell></row></table><note>MODEL SEMSEG ↑ DEPTH ↓ NORMALS ↓ ∆ MTL (%) ↑ FLOPS (G) MR↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison on PASCAL-Context. Model Semseg ↑ Normals ↓ Saliency ↑ Human ↑ Edge ↓ ∆ MTL (%) ↑ Flops (G) MR↓</figDesc><table><row><cell>STL</cell><cell>66.1</cell><cell>14.70</cell><cell>0.661</cell><cell>0.598 0.0175</cell><cell>0</cell><cell>670</cell><cell>6.0</cell></row><row><cell cols="2">MTL (uniform) 65.8</cell><cell>17.03</cell><cell>0.641</cell><cell>0.594 0.0176</cell><cell>-4.14</cell><cell>284</cell><cell>12.0</cell></row><row><cell>MTL (Scalar)</cell><cell>64.3</cell><cell>15.93</cell><cell>0.656</cell><cell>0.586 0.0172</cell><cell>-2.48</cell><cell>284</cell><cell>10.6</cell></row><row><cell>DWA</cell><cell>65.6</cell><cell>16.99</cell><cell>0.648</cell><cell>0.594 0.0180</cell><cell>-3.91</cell><cell>284</cell><cell>12.0</cell></row><row><cell>Uncertainty</cell><cell>65.5</cell><cell>17.03</cell><cell>0.651</cell><cell>0.596 0.0174</cell><cell>-3.68</cell><cell>284</cell><cell>10.2</cell></row><row><cell>RLW</cell><cell>65.2</cell><cell>17.22</cell><cell>0.660</cell><cell>0.634 0.0177</cell><cell>-2.87</cell><cell>284</cell><cell>9.2</cell></row><row><cell>PCGrad</cell><cell>62.6</cell><cell>15.35</cell><cell>0.645</cell><cell>0.596 0.0174</cell><cell>-2.58</cell><cell>284</cell><cell>12.0</cell></row><row><cell>CAGrad</cell><cell>62.3</cell><cell>15.30</cell><cell>0.648</cell><cell>0.604 0.0174</cell><cell>-2.03</cell><cell>284</cell><cell>10.2</cell></row><row><cell>MGDA-UB</cell><cell>63.0</cell><cell>15.34</cell><cell>0.646</cell><cell>0.604 0.0174</cell><cell>-1.94</cell><cell>284</cell><cell>10.2</cell></row><row><cell>Cross-stitch</cell><cell>66.3</cell><cell>15.13</cell><cell>0.663</cell><cell>0.602 0.0171</cell><cell>+0.14</cell><cell>670</cell><cell>4.0</cell></row><row><cell>MTAN</cell><cell>65.1</cell><cell>15.76</cell><cell>0.659</cell><cell>0.590 0.0170</cell><cell>-1.78</cell><cell>319</cell><cell>9.0</cell></row><row><cell>InterroGate</cell><cell>65.7</cell><cell>14.71</cell><cell>0.663</cell><cell>0.606 0.0172</cell><cell>+0.56</cell><cell>664</cell><cell>3.2</cell></row><row><cell>InterroGate</cell><cell>65.1</cell><cell>14.64</cell><cell>0.663</cell><cell>0.604 0.0172</cell><cell>+0.42</cell><cell>577</cell><cell>4.8</cell></row><row><cell>InterroGate</cell><cell>65.2</cell><cell>14.75</cell><cell>0.663</cell><cell>0.600 0.0172</cell><cell>+0.12</cell><cell>435</cell><cell>5.4</cell></row><row><cell>InterroGate</cell><cell>64.9</cell><cell>14.72</cell><cell>0.658</cell><cell>0.596 0.0172</cell><cell>-0.28</cell><cell>377</cell><cell>7.6</cell></row><row><cell>InterroGate</cell><cell>65.1</cell><cell>15.02</cell><cell>0.655</cell><cell>0.592 0.0172</cell><cell>-0.85</cell><cell>334</cell><cell>8.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Comparing the MTL performance using the L1 Hinge loss and the standard L1 loss on PASCAL-Context.</figDesc><table><row><cell>InterroGate None</cell><cell>65.7</cell><cell>14.71</cell><cell>0.663</cell><cell>0.606 0.0172</cell><cell>+0.56</cell><cell>664</cell><cell>1.8</cell></row><row><cell>InterroGate L 1</cell><cell>63.9</cell><cell>14.74</cell><cell>0.664</cell><cell>0.600 0.0172</cell><cell>-0.27</cell><cell>623</cell><cell>2.8</cell></row><row><cell>InterroGate L 1</cell><cell>61.1</cell><cell>15.07</cell><cell>0.663</cell><cell>0.582 0.0172</cell><cell>-2.20</cell><cell>518</cell><cell>4.0</cell></row><row><cell>InterroGate Hinge</cell><cell>65.1</cell><cell>14.64</cell><cell>0.648</cell><cell>0.604 0.0171</cell><cell>+0.28</cell><cell>557</cell><cell>2.2</cell></row><row><cell>InterroGate Hinge</cell><cell>65.2</cell><cell>14.75</cell><cell>0.644</cell><cell>0.600 0.0172</cell><cell>-0.13</cell><cell>433</cell><cell>3.4</cell></row></table><note>ModelL sparsity Semseg ↑Normals ↓Saliency ↑Human ↑Edge ↓∆ MTL (%) ↑Flops (G)MR↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Performance across various model capacities using the ResNet-20 and ResNet-50 backbones on the CelebA (top) and NYUD-v2 (bottom) tasks.</figDesc><table><row><cell>ORIGINAL</cell><cell>STL MTL INTERROGATE</cell><cell>97.50 97.28 97.60</cell><cell>86.02 86.70 87.44</cell><cell>93.00 92.35 92.40</cell><cell>92.17 92.11 92.48</cell><cell>174 58 59</cell><cell>2.0 2.7 1.3</cell></row><row><cell>HALF</cell><cell>STL MTL INTERROGATE</cell><cell>96.99 97.02 97.33</cell><cell>85.60 86.41 86.75</cell><cell>92.72 92.11 92.05</cell><cell>91.77 91.85 92.05</cell><cell>44.4 14.8 15.5</cell><cell>2.3 2.0 1.7</cell></row><row><cell>QUARTER</cell><cell>STL MTL INTERROGATE</cell><cell>96.64 96.46 96.81</cell><cell>85.22 85.46 86.05</cell><cell>92.19 91.59 91.48</cell><cell>91.35 91.17 91.45</cell><cell>11.6 3.9 4.7</cell><cell>2.0 2.3 1.7</cell></row><row><cell></cell><cell>MODEL</cell><cell cols="6">SEMSEG ↑ DEPTH ↓ NORMALS ↓ ∆ MTL (%) ↑ FLOPS (G) MR↓</cell></row><row><cell>ORIGINAL</cell><cell cols="2">STL MTL INTERROGATE 43.63 43.20 43.39</cell><cell>0.599 0.586 0.577</cell><cell>19.42 21.70 19.66</cell><cell>0 -3.02 +1.16</cell><cell>1149 683 892</cell><cell>2.3 2.3 1.3</cell></row><row><cell>HALF</cell><cell cols="2">STL MTL INTERROGATE 39.78 39.72 40.20</cell><cell>0.613 0.610 0.591</cell><cell>20.06 22.78 20.41</cell><cell>0 -3.98 +0.63</cell><cell>415 296 348</cell><cell>2.3 2.0 1.7</cell></row><row><cell>QUARTER</cell><cell cols="2">STL MTL INTERROGATE 35.71 35.44 35.68</cell><cell>0.654 0.632 0.624</cell><cell>21.21 24.57 21.75</cell><cell>0 -4.06 +0.94</cell><cell>177 147 164</cell><cell>2.3 2.3 1.3</cell></row><row><cell cols="8">Limitations. In this work, we primarily focus on enhanc-</cell></row><row><cell cols="8">ing the trade-off between accuracy and efficiency during</cell></row><row><cell cols="8">inference. However, InterroGate comes with a moderate</cell></row><row><cell cols="8">increase in training time. While we observe that InterroGate</cell></row><row><cell cols="8">demonstrates SoTA results on PASCAL-Context with five</cell></row><row><cell cols="8">tasks, further evaluation on MTL setups with larger number</cell></row><row><cell cols="8">of tasks remains to be explored. Furthermore, although both</cell></row><row><cell>λ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>MODELGENDER ↑ AGE ↑ CLOTHES ↑ OVERALL ↑ FLOPS (M) MR↓ s and τ t can control the trade-off between performance and computational cost, effectively approximating the desired FLOPs, we still cannot guarantee a specific target FLOP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Performance comparison of various MTL models on the CelebA dataset with different model capacities. Different InterroGate models are obtained by varying λs</figDesc><table><row><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="5">Gender ↑ Age ↑ Clothes ↑ Overall ↑ Flops (M) MR↓</cell></row><row><cell>Original</cell><cell cols="2">STL MTL InterroGate 97.60 87.44 97.50 86.02 97.28 86.70</cell><cell>93.00 92.35 92.40</cell><cell>92.17 92.11 92.48</cell><cell>174 58 59</cell><cell>3.3 4.7 2.7</cell></row><row><cell></cell><cell cols="2">InterroGate 97.77 87.39</cell><cell>92.56</cell><cell>92.57</cell><cell>11</cell><cell>2.3</cell></row><row><cell></cell><cell cols="2">InterroGate 97.95 87.24</cell><cell>92.85</cell><cell>92.68</cell><cell>162</cell><cell>2.0</cell></row><row><cell>Half</cell><cell>STL MTL</cell><cell>96.99 85.60 97.02 86.41</cell><cell>92.72 92.11</cell><cell>91.77 91.85</cell><cell>44.4 14.8</cell><cell>3.7 4.0</cell></row><row><cell></cell><cell cols="2">InterroGate 97.33 86.75</cell><cell>92.05</cell><cell>92.05</cell><cell>15.5</cell><cell>3.7</cell></row><row><cell></cell><cell cols="2">InterroGate 97.33 87.05</cell><cell>92.12</cell><cell>92.17</cell><cell>17.4</cell><cell>2.3</cell></row><row><cell></cell><cell cols="2">InterroGate 97.46 86.97</cell><cell>92.47</cell><cell>92.23</cell><cell>24.5</cell><cell>1.7</cell></row><row><cell>Quarter</cell><cell cols="2">STL MTL InterroGate 96.81 86.05 96.64 85.22 96.46 85.46</cell><cell>92.19 91.59 91.48</cell><cell>91.35 91.17 91.45</cell><cell>11.6 3.9 4.7</cell><cell>3.3 4.3 3.7</cell></row><row><cell></cell><cell cols="2">InterroGate 96.92 86.10</cell><cell>91.64</cell><cell>91.56</cell><cell>5.5</cell><cell>2.0</cell></row><row><cell></cell><cell cols="2">InterroGate 96.81 86.61</cell><cell>91.74</cell><cell>91.72</cell><cell>6.4</cell><cell>2.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Qualcomm AI Research * , Amsterdam, The Netherlands.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"> Qualcomm AI Research, Hyderabad, India.  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">XR Labs, Qualcomm Technologies Inc., Amsterdam, The Netherlands.* Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. † Work was completed while an employee at Qualcomm AI Research. Preprint. Under review.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Implementation Details <ref type="bibr">A.1. CelebA</ref> On the CelebA dataset, we use ResNet-20 as our backbone with three task-specific linear classifier heads, one for each attribute. We resize the input images to 32x32 and remove the initial pooling in the stem of ResNet to accommodate the small image resolution. For training, we use the Adam optimizer with a learning rate of 1e-3, weight decay of 1e-4, and a batch size of 128. For learning rate decay, we use a step learning rate scheduler with step size 20 and a multiplicative factor of 1/3. We use SGD with a learning rate of 0.1 for the gates' parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. NYUD and PASCAL-Context</head><p>For both NYUD-v2 and PASCAL-Context with ResNet-18 and ResNet-50 backbones, we use the Atrous Spatial Pyramid Pooling (ASPP) module introduced by <ref type="bibr" target="#b5">(Chen et al., 2018a)</ref> as task-specific heads. For the HRNet-18 backbone, we follow the methodology of the original paper <ref type="bibr" target="#b40">(Wang et al., 2020)</ref>: HRNet combines the output representations at four different resolutions and fuses them using 1x1 convolutions to output dense prediction.</p><p>We train all convolution-based encoders on the NYUD-v2 dataset for 100 epochs with a batch size of 4 and on the PASCAL-Context dataset for 60 epochs with a batch size of 8. We use the Adam optimizer, with a learning rate of 1e-4 and weight decay of 1e-4. We use the same data augmentation strategies for both NYUD-v2 and PASCAL-Context datasets as described in <ref type="bibr" target="#b37">(Vandenhende et al., 2020)</ref>. We use SGD with a learning rate of 0.1 to learn the gates' parameters.</p><p>In terms of task objectives, we use the cross-entropy loss for semantic segmentation and human parts, L 1 loss for depth and normals, and binary-cross entropy loss for edge and saliency detection tasks, similar to <ref type="bibr" target="#b37">(Vandenhende et al., 2020)</ref>. For learning rate decay, we adopt a polynomial learning rate decay scheme with a power of 0.9.</p><p>The Choice of ω t . The hyper-parameter ω t denotes the scalarization weights. We use the weights suggested in prior work but also report numbers of uniform scalarization. For NYUD-v2, we use uniform scalarization as suggested in <ref type="bibr" target="#b26">(Maninis et al., 2019;</ref><ref type="bibr" target="#b38">Vandenhende et al., 2021)</ref>, and for PASCAL-Context, we similarly use the weights suggested in <ref type="bibr" target="#b26">(Maninis et al., 2019)</ref> and <ref type="bibr" target="#b38">(Vandenhende et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. DPT Training</head><p>For DPT training, we follow the same training procedure as described by the authors, which employs the Adam optimizer, with a learning rate of 1e-5 for the encoder and 1e-4 for the task heads, and a batch size of 8.</p><p>The ViT backbones were pre-trained on ImageNet-21k at resolution 224×224, and fine-tuned on ImageNet 2012 at resolution 384×384. The feature dimension for DPT's task heads was reduced from 256 to 64. We conducted a sweep over a set of weight decay values and chose 1e-6 as the optimal value for our DPT experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Forward-pass Pseudo-code</head><p>Algorithm 1 illustrates the steps in the forward pass of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization to Vision Transformers</head><p>As transformers are becoming widely used in the vision literature, and to show the generality of our proposed MTL framework, we also apply InterroGate to vision transformers: We again denote φ ℓ t ∈ R N ℓ ×C ℓ and ψ ℓ ∈ R N ℓ ×C ℓ as the t-th task-specific and shared representations in layer ℓ, where N ℓ and C ℓ are the number of tokens and embedding dimensions, respectively. We first apply our feature selection to the key, query and value linear projections in each self-attention block: Figure <ref type="figure">4</ref> illustrates the distribution of the gating patterns across all layers of the ResNet-18 backbone for the PASCAL-Context dataset for 3 models using (a) a Hinge loss, (b) a medium-level pruning using uniform L 1 loss and (c) a high-level pruning with uniform L 1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation: Sparsity Targets</head><p>By tuning the sparsity targets τ in Equation <ref type="formula">4</ref>, we can achieve specific compute budgets of the final network at inference. However, there are multiple choices of {τ t } T t=1 that can achieve the same budget. In this section, we further investigate the impact of which task we allocate more or less of the compute budget on the final accuracy/efficiency trade-off.</p><p>We perform an experiment sweep for different combination of sparsity targets, where each τ t is chosen from {0, 0.25, 0.75, 1.0}. Each experiment is run for two different random seeds and two different sparsity loss weights λ s . Due to the large number of experiments, we perform the ablation experiments for shorter training runs (75% of the training epochs for each setup) Our take-away conclusions are that (i) we clearly observe that some tasks require more task-specific parameters (hence a higher sparsity target) and (ii) this dichotomy often correlates with the per-task performance gap observed between the STL and MTL baselines, which can thus be used as a guide to set the hyperparameter values for τ .</p><p>In the results of NYUD-v2 in Figure <ref type="figure">5</ref>, we observe a clear hierarchy in terms of task importance: When looking at the points on the Pareto curve, they prefer high values of τ normals , followed by τ segmentation : In other words, these two tasks, and in particular normals prediction, requires more task-specific parameters than the depth prediction task to obtain the best MTL performance versus compute cost trade-offs.</p><p>Then, we conduct a similar analysis for the five tasks of PASCAL-Context in Figure <ref type="figure">6</ref>. Here we see a clear split in tasks: The graph for the edges prediction and saliency task are very similar to one another and tend to prefer high τ values, i.e. more task-specific parameters, at higher compute budget. But when focusing on a lower compute budget, it is more beneficial to the overall objective for these tasks to use the shared branch. Similarly, the tasks of segmentation and human parts exhibit similar behavior under variations of τ and are more robust to using shared representations (lower values of τ ). Finally, the task of normals prediction (b) differ from the other four, and in particular exhibit a variance of behavior across different compute budget. In particular, when targetting the intermediate range (350B-450B FLOPs), setting higher τ normals helps the overall objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training Time Comparisons</head><p>While our method is mainly aiming at improving the inference cost efficiency, we also measure and compare training times between our method and the baselines, on the PASCAL-Context <ref type="bibr" target="#b6">(Chen et al., 2014)</ref>. The results are shown on Table <ref type="table">11</ref>. The forward and backward iterations are averaged over 1000 iterations, after 10 warmup iterations, on a single NvidiaV100 GPU, with a batch size of 4. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Budget-aware adapters for multi-domain learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mult: An end-to-end multitask learning transformer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12031" to="12041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1385" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring relational context for multitask dense prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brüggemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15869" to="15878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Just pick a sign: Optimizing deep multitask models with gradient sign dropout</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2039" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mod-squad: Designing mixtures of experts as modular multi-task learners</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11828" to="11837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">M 3 vit: Mixture-of-experts vision transformer for efficient multi-task learning with modelaccelerator co-design</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="28441" to="28457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficiently identifying task groupings for multi-task learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27503" to="27516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mtlnas: Task-agnostic neural architecture search towards general-purpose multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11543" to="11552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to branch for multi-task learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3854" to="3863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hazimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29335" to="29347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rotograd: Gradient homogenization in multitask learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Javaloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reasonable effectiveness of random weighting: A litmus test for multi-task learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feiyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Machine Learning Research</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conflictaverse gradient descent for multi-task learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18878" to="18890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-lambda: Disentangling dynamic task relationships</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Machine Learning Research</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE international Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
				<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><surname>Piggyback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Achituve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<title level="m">Multi-task learning as a bargaining game. International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16428" to="16446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynashare: Task and instance conditioned parameter sharing for multi-task learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahimian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalarization for multi-task and multi-domain learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Edgemoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18691</idno>
		<title level="m">Memory-efficient multi-task vision transformer architecture with task-level sparsity via mixture-of-experts</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-task learning as multiobjective optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9120" to="9132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning what to share for efficient deep multi-task learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><surname>Adashare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8728" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mtinet: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">;</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proesmans</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3614" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Task adaptive parameter sharing for multi-task learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7561" to="7570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11293" to="11302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Edgemoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.14352</idno>
		<title level="m">Fast on-device inference of moe-based large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5824" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A modulation module for multi-task learning with applications in image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
