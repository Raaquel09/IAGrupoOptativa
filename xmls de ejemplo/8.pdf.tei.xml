<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-02-26">26 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liuzhenghao</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongying</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuyang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu-Chian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
						</author>
						<title level="a" type="main">ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-26">26 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">759B0FC08DF1C8DDE55DE00AF4EEC2F7</idno>
					<idno type="arXiv">arXiv:2402.16445v1[cs.CE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-06T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs), including GPTx and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Large Language Models (ProLLMs) trained on protein corpora excel at de novo protein sequence generation. However, as of now, unlike LLMs in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a ProLLM capable of handling multiple PLP tasks. Specifically, our framework utilizes low-rank adaptation and employs a twostage training approach, and it is distinguished by its universality, low overhead, and scalability. Through training under this framework, we propose the ProLLaMA model, the first known Pro-LLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves stateof-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. In the protein property prediction task, ProLLaMA achieves nearly 100% accuracy across many categories. The latter two tasks are beyond the reach of other ProLLMs. Code is available at https: //github.com/Lyu6PosHao/ProLLaMA. tein LLMs (ProLLMs) have paved the way for the rapid generation of structurally plausible protein sequences, holding immense potential for biomedical and biotechnological innovations <ref type="bibr" target="#b32">(Notin et al., 2022;</ref><ref type="bibr" target="#b31">Nijkamp et al., 2023)</ref>. However, this progress is met with challenges, particularly in extending their capabilities beyond sequence generation.</p><p>Analogous to NLP, tasks related to protein language can be viewed as Protein Language Processing (PLP) <ref type="bibr" target="#b5">(Bepler &amp; Berger, 2021;</ref><ref type="bibr" target="#b33">Ofer et al., 2021)</ref>. It is evident that current ProLLMs primarily focus on a single task in PLP, not covering multiple tasks like LLMs do in NLP <ref type="bibr">(Ferruz et al., 2022;</ref><ref type="bibr" target="#b27">Madani et al., 2023)</ref>. These limitations prompt the need for innovative solutions to unleash the full potential of ProLLMs. Developing a multi-tasking ProLLM would be highly beneficial for protein engineering and understanding the protein fitness landscape <ref type="bibr" target="#b48">(Wright et al., 1932;</ref><ref type="bibr" target="#b34">Pan &amp; Kortemme, 2021;</ref><ref type="bibr" target="#b38">Ren et al., 2022;</ref><ref type="bibr" target="#b40">Song &amp; Li, 2023)</ref>, but three main challenges must be considered: (i) Necessity of Natural Language: Protein language is not fully sufficient for PLP tasks, meaning it cannot fully represent all components of a task (the user instruction and the expected output) <ref type="bibr" target="#b49">(Xu et al., 2023;</ref><ref type="bibr" target="#b47">Wang et al., 2023)</ref>. The instruction and the output require a language beyond protein language (typically, natural language) for representation, which current ProLLMs lack. (ii) Instruction Following: To possess multi-tasking capabilities, models must execute tasks following user instructions <ref type="bibr" target="#b51">(Zeng et al., 2023;</ref><ref type="bibr" target="#b26">Liu et al., 2023b;</ref><ref type="bibr" target="#b57">Zhou et al., 2023)</ref>. However, current ProLLMs are unable to follow instructions.</p><p>(iii) Training Resource Consumption: Substantial training resources are needed for models to learn natural language, protein language, and user instructions <ref type="bibr" target="#b9">(Cui et al., 2023)</ref>, which can sometimes be unaffordable.</p><p>To address the challenges, we propose a two-stage training framework to achieve a ProLLM for multi-task PLP. In the first stage, we leverage a pre-trained general LLM like LLaMA2 to continually learn the protein language while maintaining the natural language knowledge of the model. In the second stage, the model is further trained on a multitask PLP dataset through instruction tuning to equip the model with instruction following capabilities and multi-task capabilities of PLP. Notably, during both stages, we adopt Low-Rank Adaptation (LoRA) <ref type="bibr" target="#b17">(Hu et al., 2021)</ref>, which prevents catastrophic forgetting in the first stage, enhances scalability in the second stage and reduces the training cost.</p><p>Using the training framework, we develop ProLLaMA, a model capable of multi-tasks for PLP, distinguishing it from all other ProLLMs. Through a series of experiments, we demonstrate the multitasking capabilities of our ProLLaMA. Specifically, in unconditional protein generation, ProLLaMA outperforms current ProLLMs and other sequence-generative models on common metrics such as pLDDT and TM-score. In controllable protein generation, based on a user-provided textual description of the desired protein functionalities, ProLLaMA generates novel proteins from scratch with functionalities as competent as natural proteins, such as SAM-MT and Trx. For protein property prediction, ProLLaMA achieves an average accuracy of 72% on the test dataset and nearly 100% accuracy in many subcategories. In summary, the contributions of our research are as follows:</p><p>• We propose a training framework with scalability and efficiency that enables any general LLM to be trained as a proficient model for multiple tasks in Protein Language Processing.</p><p>• Our ProLLaMA is the first model to our knowledge capable of simultaneously handling multiple PLP tasks.</p><p>• Experiments show that our ProLLaMA not only handles PLP tasks beyond the reach of existing ProLLMs but also achieves state-of-the-art results in the protein generation task where current ProLLMs are active.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Necessity of Natural Language</head><p>As aforementioned, given the similarities between protein sequences and natural language, tasks related to protein sequences can be considered as Protein Language Processing (PLP), analogous to NLP. However, we observe a fundamental difference between protein language and natural language: natural language is complete for NLP tasks, whereas protein language is not complete for PLP tasks. Specifically, natural language can represent all components of a task (i.e., the input instructions and the expected output) <ref type="bibr" target="#b24">(Liu et al., 2023a)</ref>. For example, in a sentiment analysis task, the instruction and output can be expressed as "Analyze the sentiment contained in this sentence: I am happy" and "The sentiment is positive" respectively, all in natural language. However, for PLP tasks, the instructions and outputs cannot be fully represented in protein language. For instance, in the protein property prediction task, the task instruction could be "Predict the property of this protein: MAFCF...FEV", with the expected output being "The property is Trx superfamily." It is evident that both the instructions and outputs of tasks require assistance from a language beyond protein language, in this case, natural language, for representation. Therefore, multi-task ProLLMs must possess a certain level of natural language ability, especially as more textual descriptions of proteins become available <ref type="bibr" target="#b49">(Xu et al., 2023)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Causal Language Modeling</head><p>Causal Language Modeling (CLM) is the objective of training LLMs. Given a token sequence x = {x 0 , x 1 , . . . , x n−1 } that is subject to training. CLM can be conceptualized as predicting the i-th token based on the preceding i − 1 tokens <ref type="bibr" target="#b4">(Bengio et al., 2000)</ref>. Therefore, the optimization objective when training LLM using CLM is formulated as:</p><formula xml:id="formula_0">L(Θ) = Ex∼D − i log p(xi|x0, x1, . . . , xi−1; Θ) (1)</formula><p>where L denotes the loss function, Θ denotes the trainable parameters of the model, and D is the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Low Rank Adaptation</head><p>Low-Rank Adaptation (LoRA) <ref type="bibr" target="#b17">(Hu et al., 2021</ref>) is a parameter-efficient technique for fine-tuning LLMs. Due to the immense parameter size of LLMs, full-parameter finetuning could be impractical sometimes. LoRA circumvents this by freezing the original parameters of LLMs and introducing additional trainable low-rank adapters. It achieves fine-tuning with a significantly smaller number of trainable parameters, yielding results comparable to full-parameter fine-tuning.</p><p>Theoretically, fine-tuning can be conceptualized as a process of finding the change in parameters <ref type="bibr" target="#b12">(Ding et al., 2023)</ref>. Let the original parameters of the model be denoted as W 0 , and the parameters after fine-tuning as W. The objective of finetuning is to find ∆W = W − W 0 . Hypothesizing ∆W is of low rank <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref>, denoted as r, it can be decomposed into two low-rank matrices, ∆W = AB. This leads to the following equation:</p><formula xml:id="formula_1">W = W 0 + AB<label>(2)</label></formula><p>where W, W 0 ∈ R d×h , A ∈ R d×r , and B ∈ R r×h . And the fine-tuning objective is transformed from finding ∆W to finding A and B:</p><formula xml:id="formula_2">min ∆W L(∆W) → min A,B L(A, B)<label>(3)</label></formula><p>Consequently, the quantity of parameters involved in training is reduced from dh to r(d + h). Given the low-rank hypothesis, where r ≪ d and r ≪ h, this reduction in the number of trainable parameters is quite significant.</p><p>During training, the original parameters of the model are frozen, with only the low-rank adapters trainable. After training, the newly acquired knowledge is stored in the low-rank adapters, namely A and B. Additionally, A and B can be integrated into the original model using Equation <ref type="formula" target="#formula_1">2</ref>. Although this integration means the low-rank adapters are no longer plug-and-play, it ensures that the architecture of the post-training model remains consistent with that of the original model. Finally, LoRA prevents catastrophic forgetting of the original knowledge, as the newly learned knowledge has a lower rank than the original knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>The overview of the architecture of our ProLLaMA is shown in Figure <ref type="figure" target="#fig_0">2</ref>, and the overview of our training framework is shown in Figure <ref type="figure" target="#fig_1">3</ref>. In Section 3.1, we show how the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLaMA2</head><p>ProLLaMA ProLLaMA  LLaMA2 model learns protein language through continual learning, resulting in ProLLaMA. In Section 3.2, we show the integration of various tasks into ProLLaMA through instruction tuning. In Section 3.3, we show more tasks can be integrated into ProLLaMA. These three sections correspond to Figure <ref type="figure" target="#fig_1">3(A-C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Continual Learning on Protein Language</head><p>As mentioned in Section 1, current ProLLMs are lack of natural language abilities, which hinders multi-task capabilities. To solve this problem, we propose leveraging a pre-trained LLaMA2 to perform continual learning on protein language, as shown in Figure <ref type="figure" target="#fig_1">3(A)</ref>. This approach is analogous to humans learning a foreign language, where the model learns protein language while retaining its original natural language abilities. Specifically, we construct a dataset based on UniRef50 <ref type="bibr" target="#b42">(Suzek et al., 2015)</ref>. We preprocess each protein sequence with specific prefixes and suffixes. This standardized format aids LLaMA2 in distinguishing the new protein language from its existing natural language knowledge, thereby reducing confusion.</p><p>We add low-rank adapters (LoRA) into LLaMA2. To be specific, in each Decoder layer of LLaMA2, we add LoRA to certain weights including W q , W k , W v , W o , W up , W gate and W down . The original parameters of LLaMA2 are frozen, enabling only LoRA to be trained. Due to the significant differences between protein language and natural language, we choose a relatively high rank for LoRA, which helps the model learn protein sequences more effectively and prevents under-fitting. We include both the Embed and Generation Head layers in training. This is based on the premise that a token may have different meanings in protein sequences and natural languages, requiring distinct embeddings for the same token.</p><p>We train the model with Equation 1 as the target on the aforementioned dataset, resulting in ProLLaMA. Benefiting from LoRA, we train only 10% of the parameters, in contrast to full-parameter training, which significantly reduces training costs. Additionally, as the remaining parameters are not involved in training, the inherent natural language abilities of the model are preserved.</p><p>In summary, by enabling a pre-trained LLaMA2 to continually learn protein language and utilizing LoRA, we have developed ProLLaMA, a model that comprehends both protein sequence and natural language. Consequently, we have addressed the problem mentioned in the introduction: the lack of natural language abilities and excessive consumption of training resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performing Various Tasks</head><p>As mentioned in Section 1, current ProLLMs are unable to perform multiple tasks based on user instructions. To solve this problem, we perform instruction tuning on the ProLLaMA obtained from the previous section, as shown in Figure <ref type="figure" target="#fig_1">3(B)</ref>.</p><p>To be specific, we construct a multi-task dataset, where each item represents a task sample. A task consists of two parts: the instruction and the output. Training conducted on this dataset can be denoted as follows:</p><formula xml:id="formula_3">L(Θ) = E x,u∼D [− log p(x|u; Θ)]<label>(4)</label></formula><p>Here, Θ denotes the parameters to be optimized, L the loss function, and D the dataset. u denotes the instruction of a task. x = {x 0 , x 1 , . . . , x n−1 } denotes the output of a task, where x i is the i-th token of the output.</p><p>Since ProLLaMA employs causal language modeling, we need to combine Equation <ref type="formula" target="#formula_3">4</ref>with Equation <ref type="formula">1</ref>:</p><formula xml:id="formula_4">L(Θ) = Ex∼D − i log p(xi|u, x0, x1, . . . , xi−1; Θ) (5)</formula><p>Equation 5 is the optimization objective for instruction tuning of ProLLaMA. u is not involved in the loss calculation, whereas x is. This is because the latter is the output part, where ensuring its quality of generation is crucial. The former, u, as an instruction signal, only needs to be understood by the model and does not require generation. In the instruc-tion tuning stage, we exclusively train LoRA at a lower rank than specified in Section 3.1.</p><p>In summary, through instruction tuning, we have made Pro-LLaMA capable of following instructions and performing multiple tasks. In contrast, other ProLLMs remain limited to the single task of protein sequence generation. Consequently, we have addressed the problems mentioned in the introduction: the lack of instruction following and the lack of multi-task capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Expanding to More Tasks</head><p>In this section, we demonstrate that benefiting from the scalability of the training framework, ProLLaMA can be easily extended to a broader range of tasks as shown in Figure <ref type="figure" target="#fig_1">3</ref>(C).</p><p>To be specific, to adapt ProLLaMA to more tasks, researchers must customize the instruction dataset and then perform instruction tuning on ProLLaMA again with this dataset. ProLLaMA can be either the one obtained in the first stage or the one obtained in the second stage. Benefiting from LoRA, such instruction tuning requires only minimal training resources. An instance is shown in Figure <ref type="figure" target="#fig_1">3</ref>(C).</p><p>The new task in this instance is protein complex prediction, where the model must ascertain whether two input proteins can form a complex. In this task, the instruction u comprises a sentence describing the user intent and two protein sequences, while the output x should be a judgment of Yes or No. In summary, our framework and model are flexible and easily extensible. With collaborative efforts from other researchers, ProLLaMA has the potential to become more powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We introduce the experiment setup in Section 4.1. And we evaluate the unconditional protein generation task in Section 4.2, the controllable protein generation task in Section 4.3, the protein property prediction task in Section 4.4, and the natural language ability in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Training Settings: For continual learning, the LoRA rank is set to 128, employing the AdamW optimizer alongside a cosine annealing scheduler with warm-up. The peak learning rate stands at 0.05, with a total of one training epoch. For instruction tuning, the LoRA rank is set to 64 with two training epochs, and all other settings remain consistent with the continual learning setup.</p><p>Training Datasets: For continual learning, the dataset used consists of protein sequences from UniRef50 <ref type="bibr" target="#b42">(Suzek et al., 2015)</ref>. For instruction tuning, the instruction dataset is constructed using protein sequences from UniRef50 and protein property texts from InterPro (Paysan-Lafosse et al., 2023).</p><p>Evaluation Settings: Unconditional protein generation involves generating protein sequences without specific instructions. Controllable protein generation involves generating desired protein sequences based on instructions that specify the required functionalities. Property prediction involves predicting protein properties based on instructions, which include the protein sequences to be predicted.</p><p>Evaluation Metrics: We use the following metrics to evaluate the generated protein sequences. The pLDDT <ref type="bibr" target="#b20">(Jumper et al., 2021)</ref> is used to measure whether sequences are structurally plausible. Self-Consistency Perplexity (SC-Perp) <ref type="bibr" target="#b1">(Alamdari et al., 2023)</ref> serves as an additional metric of plausible structures since pLDDT falls short in dealing with intrinsically disordered regions (IDRs) <ref type="bibr" target="#b10">(Davey, 2019)</ref>. TM-score <ref type="bibr" target="#b53">(Zhang &amp; Skolnick, 2004)</ref> reflects the structural similarity between generated sequences and known ones in AFDB <ref type="bibr" target="#b45">(Varadi et al., 2022)</ref> and PDB <ref type="bibr" target="#b6">(Berman et al., 2002)</ref>. RMSD also reflects the structural similarity from the perspective of atomic distance. Homologous probability (H-Prob) reflects the probability that the generated protein is homologous to a known one. Seq-Ident reflects the sequence similarity between generated sequences and known ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Unconditional Protein Generation</head><p>We compare our model with other state-of-the-art models in the protein sequence generation field. These models encompass a wide range of architectures and are trained exclusively on protein sequences. Table <ref type="table" target="#tab_4">3</ref>.2 shows the results. Our ProLLaMA is optimal on pLDDT, TM-score, and RMSD and is suboptimal on SC-Perp. This indicates that ProLLaMA, through its training on protein sequence data, can generate structurally plausible proteins. Additionally, it is important to note that ProLLaMA-generated proteins exhibit a mean and standard deviation for pLDDT and SC-Perp of 66.49±12.61 and 3.10±0.65, respectively. These values are comparable to those of natural proteins as reported by <ref type="bibr" target="#b1">Alamdari et al. (2023)</ref>, which are 68.25±17.85 and 3.09±0.63, respectively.</p><p>De novo design of long and structurally plausible protein sequences is highly challenging <ref type="bibr">(Ferruz et al., 2022</ref>), yet our ProLLaMA excels in meeting this challenge. Figure <ref type="figure" target="#fig_3">4(A)</ref> shows that as the sequence length increases, the pLDDT of proteins generated by ProLLaMA does not decrease but rather increases. In contrast, the pLDDT of proteins generated by ESM2 significantly decreases as the length increases. This indicates that ProLLaMA is able to capture long-range dependencies between amino acids, while ESM2 faces difficulties in the de novo design of long protein sequences.  shows that as the length increases, SC-Perp of ProLLaMA slightly increases and then continues to decrease, while SC-Perp of ESM2 does the opposite. Figure <ref type="figure" target="#fig_3">4</ref>(C) shows that as the length increases, the TM-score of ESM2 gradually decreases from above 0.6 to less than 0.2. In contrast, the TM-score of ProLLaMA remains above 0.5 for nearly all lengths, even exceeding 0.8 when the length is greater than 350. These demonstrate the robust sequence generation capability of ProLLaMA, especially in generating longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Controllable Protein Generation</head><p>To instruct ProLLaMA in controllable protein generation, we utilize four superfamily descriptions as instructions respectively: the S-adenosyl-L-methionine-dependent methyltransferase superfamily (SAM-MT), the Tetratricopeptidelike helical domain superfamily (TPHD), the Thioredoxinlike superfamily (Trx), and the CheY-like superfamily (CheY). For each superfamily, ProLLaMA generates 100 protein sequences. Additionally, we use 100 natural proteins from each of the four superfamilies as benchmarks for comparison. We employ Foldseek to compare the generated proteins with the natural ones. The results shown in Table <ref type="table" target="#tab_2">2</ref> demonstrate that ProLLaMA can generate desired protein sequences based on instructions that specify the required functionalities, confirming the capability for controllable generation. For SAM-MT, the TM-scores of our generated sequences exceed 0.7; for TPHD and CheY, they are over 0.8; and for Trx, they surpass 0.9. The high TM-score indicates that the structures of the generated proteins closely resemble those of natural proteins in the same superfamily, implying functional similarity. For SAM-MT, TPHD, Trx, and CheY, all of the H-prob values are close to or even equal to 100%, indicating that the generated proteins are homologous to natural proteins and belong to the same superfamily. In summary, these provide strong evidence that the protein generation of ProLLaMA is controllable under instructions.</p><p>In contrast, other models exhibit low TM-score and very low H-Prob due to their uncontrollable generation.</p><p>Additionally, using natural proteins as a benchmark, we assess pLDDT, SC-Perp, and TM-score of proteins generated by ProLLaMA. Figure <ref type="figure" target="#fig_3">4</ref>   Trx, and SAM-MT, the average pLDDT of generated proteins is only 19.0%, 1.41%, 10.9%, and 10.3% lower than that of natural proteins, respectively. Figure <ref type="figure" target="#fig_3">4</ref>(E) shows that for TPHD and SAM-MT, the average SC-Perp is 5.66% and 5.06% lower; for Trx and CheY, the average SC-Perp is 7.58% and 14.92% higher. Figure <ref type="figure" target="#fig_3">4</ref>(F) visualizes the TM-score, with scores near the maximum indicating a high degree of structural similarity. These findings indicate that proteins generated by ProLLaMA are comparable to their natural counterparts in the same superfamily. Given that averages are considered, there are instances where generated proteins outperform natural proteins.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref>, we present actual examples of proteins generated by ProLLaMA (depicted in blue) alongside the most structurally similar natural proteins from PDB (depicted in yellow). The four proteins are generated under the instructions of SAM-MT, TPHD, Trx, and CheY. We find that the four natural proteins also belong to SAM-MT, TPHD, Trx, and CheY, respectively, according to information from InterPro. This implies functional similarity between the generated proteins and natural ones, which further validates the effectiveness of controllable generation. The significant overlap in the 3D structures and the high TM-score confirm structural similarity. The lower Seq-Ident scores indicate sequence diversity. In summary, through controllable pro-tein generation, ProLLaMA is capable of generating desired proteins with functions and structures similar to natural proteins, yet with novel sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Property Prediction</head><p>We use the test dataset to evaluate whether ProLLaMA can predict the superfamily to which a given protein belongs.</p><p>The test dataset consists of 10,000 samples. One protein may belong to two or more superfamilies simultaneously. Therefore, for the i-th protein in the test dataset, we denote its true superfamily text description as the set F i and the prediction by ProLLaMA as the set F ′ i . We calculate the prediction accuracy using:</p><formula xml:id="formula_5">Acc = N i=1 |F i ∩ F ′ i | N i=1 |F ′ i | (6)</formula><p>where |X| denotes the number of elements in set X, ∩ denotes the intersection of two sets, and N denotes the number of samples in the test dataset.</p><p>Although ProLLaMA actually performs a classification task here, it is more complex than typical ones. The key difference is that typical classification tasks require models to output a fixed label, often in one-hot encoding. In contrast, ProLLaMA outputs a full textual description of the result,  significantly increasing the challenge. Even so, ProLLaMA still achieves an average accuracy of 72% on the test dataset.</p><p>In addition, ProLLaMA achieves considerably high accuracy in many specific superfamily categories. Table <ref type="table" target="#tab_4">3</ref> lists ten of them, showing that the accuracy exceeds 90% in all the ten superfamilies and reaches 100% in OBFD, UPF0145, NACD, U3S, Kazal, and CheY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Natural Language Ability</head><p>We evaluate the natural language ability of ProLLaMA and other models. As shown in Table <ref type="table" target="#tab_5">4</ref>, the vocabulary of Pro-Gen2 only includes uppercase letters representing amino acids, failing to cover all English words and phrases. As for the sentence generation task on Wikipedia text, the accuracy of ProtGPT2 is 0%, indicating that it lacks natural language abilities. The accuracy of ProLLaMA is 26%, compared to 45% of LLaMA2. As for the QA task, the accuracy of ProLLaMA is 33%, compared to 44% of LLaMA2. These suggest that ProLLaMA has natural language abilities, albeit at a reduced level compared to LLaMA2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work Protein Language Models</head><p>ProLLMs aforementioned are a subset of Protein Language Models (PLMs). Recognizing the similarity between natural language sequences and protein sequences, many algorithms from NLP have been applied to protein sequence data <ref type="bibr" target="#b50">(Yang et al., 2018;</ref><ref type="bibr" target="#b2">Alley et al., 2019;</ref><ref type="bibr" target="#b37">Rao et al., 2021;</ref><ref type="bibr" target="#b13">Elnaggar et al., 2021)</ref>. This has led to the development of PLMs, which are broadly categorized into two types <ref type="bibr">(Ferruz &amp; Höcker, 2022;</ref><ref type="bibr" target="#b55">Zheng et al., 2023)</ref>.</p><p>One type adopts the decoder-only architecture and Causal Language Modeling (CLM) <ref type="bibr" target="#b4">(Bengio et al., 2000;</ref><ref type="bibr" target="#b46">Vaswani et al., 2017)</ref>, similar to general LLMs. Therefore, we refer to it as the Protein Large Language Model (ProLLM). Pro-LLMs primarily concentrate on de novo protein sequence generation <ref type="bibr" target="#b30">(Moffat et al., 2022;</ref><ref type="bibr">Ferruz et al., 2022;</ref><ref type="bibr" target="#b27">Madani et al., 2023;</ref><ref type="bibr" target="#b31">Nijkamp et al., 2023)</ref>, with a minority also focusing on fitness prediction <ref type="bibr" target="#b32">(Notin et al., 2022)</ref>. The other type adopts the encoder-only architecture and Masked Language Modeling (MLM) <ref type="bibr" target="#b11">(Devlin et al., 2018;</ref><ref type="bibr" target="#b28">Meier et al., 2021;</ref><ref type="bibr" target="#b39">Rives et al., 2021;</ref><ref type="bibr" target="#b7">Brandes et al., 2022;</ref><ref type="bibr" target="#b23">Lin et al., 2023)</ref>. They excel in protein representation learning, with the learned representations being applied to downstream tasks such as property prediction <ref type="bibr" target="#b49">(Xu et al., 2023)</ref>. However, they face challenges in de novo protein design.</p><p>Our ProLLaMA is capable of multitasking, excelling in tasks that both of the above types specialize in, surpassing existing ProLLMs and even PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training LLMs</head><p>There is a well-established procedure for training LLMs <ref type="bibr" target="#b54">(Zhao et al., 2023)</ref>. Initially, models undergo pretraining on large-scale, unlabeled corpora to grasp grammatical and semantic principles <ref type="bibr" target="#b29">(Min et al., 2023)</ref>. Then, models are subjected to instruction tuning using an instruction dataset, enabling models to understand instructions and perform various tasks <ref type="bibr" target="#b52">(Zhang et al., 2023)</ref>. However, current ProLLMs cannot undergo instruction tuning due to the lack of natural language abilities.</p><p>Various parameter-efficient techniques have been proposed to accelerate training and conserve memory <ref type="bibr" target="#b16">(He et al., 2021;</ref><ref type="bibr" target="#b22">Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b17">Hu et al., 2021;</ref><ref type="bibr" target="#b25">Liu et al., 2022;</ref><ref type="bibr" target="#b18">Hu et al., 2023)</ref>, like Low-rank Adaptation (LoRA). We propose the use of LoRA in our training framework, which prevents catastrophic forgetting, enhances scalability, and reduces training costs. To our knowledge, this is the first application of parameter-efficient training techniques for ProLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Existing ProLLMs excel in the single task of protein generation but fall short in multiple tasks. In this work, we introduce an efficient training framework to transform any general LLM into a multi-task ProLLM. We also develop ProLLaMA, a versatile ProLLM for multiple tasks like con-trollable protein generation and protein property prediction. Experiments indicate that ProLLaMA performs exceptionally well. We are confident that our framework and model will have a significant impact on the AI4Science community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statements</head><p>Our work holds the potential to revolutionize the field of computational biology and biotechnology. With multi-task capabilities in the protein field, our ProLLaMA could significantly accelerate the pace of research and innovation in areas such as drug discovery, synthetic biology, and the development of novel biomaterials.</p><p>Moreover, our work can enable a wider range of researchers and institutions, particularly those with limited resources, to participate in cutting-edge research, fostering a more inclusive AI4Science community.</p><p>However, our work may raise concerns regarding model reliability and the potential for misuse. The uncertainty in model outputs could lead to misinformed decisions in critical research areas like drug discovery, necessitating cautious reliance and thorough validation by human experts to mitigate risks. Moreover, the powerful capabilities of ProLLaMA could be exploited for harmful purposes, posing bio-security challenges and ethical challenges.</p><p>A collaborative effort among researchers, policymakers, and the broader community is crucial to harness the benefits of our work while addressing the potential risks and ethical considerations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The overall architecture of our ProLLaMA. We add low-rank adapters (LoRA) to certain weights. During the training process, we freeze these weights and other parameters, focusing solely on training LoRA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The overall training framework of our ProLLaMA. (A-B): Training is divided into two stages. (A): By continual learning on UniRef50, LLaMA2 learns protein language, resulting in ProLLaMA. (B) By instruction tuning on the instruction dataset, ProLLaMA is capable of multi-tasking. (C) By instruction tuning again on your own dataset, ProLLaMA can be expanded to more tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4(B) and Figure 4(C) also support this point. Figure 4(B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Contrast experiment of ProLLaMA. (A-C): Compared to the baseline (ESM2), ProLLaMA maintains a high, and even higher, quality of generated proteins as their length increases. (D-F): For the four superfamilies of CheY, TPHD, Trx, and SAM-MT (SAM), the proteins generated by ProLLaMA are comparable to natural proteins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Protein visualization. Cases of controllable generation by ProLLaMA using SAM-MT, TPHD, Trx, and CheY as instructions. Blue is generated proteins, and yellow is natural. They are similar in structure and function but different in sequence.</figDesc><graphic coords="8,311.23,73.24,53.14,58.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of proteins generated by different models. Our ProLLaMA achieves the best performance on pLDDT, TM-score, and RMSD metrics, and is second-best in SC-Perp, demonstrating ProLLaMA excels in de novo protein design.</figDesc><table><row><cell>Architecture</cell><cell>Method</cell><cell>pLDDT↑</cell><cell>SC-Perp↓</cell><cell>AFDB</cell><cell></cell><cell>PDB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">TM-score↑ RMSD↓ TM-score↑ RMSD↓</cell></row><row><cell>CNN</cell><cell>CARP (Alamdari et al., 2023) LRAR (Alamdari et al., 2023)</cell><cell cols="2">34.40±14.43 4.05±0.52 49.13±15.50 3.59±0.54</cell><cell>0.28 0.40</cell><cell>19.38 14.47</cell><cell>0.38 0.43</cell><cell>8.95 9.47</cell></row><row><cell>AutoEncoder</cell><cell>ESM-1b (Rives et al., 2021) ESM-2 (Lin et al., 2023)</cell><cell cols="2">59.57±15.36 3.47±0.68 51.16±15.52 3.58±0.69</cell><cell>0.34 0.20</cell><cell>20.88 35.70</cell><cell>0.44 0.41</cell><cell>8.59 9.57</cell></row><row><cell>Diffusion</cell><cell cols="3">EvoDiff (Alamdari et al., 2023) 44.29±14.51 3.71±0.52</cell><cell>0.32</cell><cell>21.02</cell><cell>0.41</cell><cell>10.11</cell></row><row><cell></cell><cell>ProtGPT2 (Ferruz et al., 2022)</cell><cell cols="2">56.32±16.05 3.27±0.59</cell><cell>0.44</cell><cell>12.60</cell><cell>0.43</cell><cell>9.19</cell></row><row><cell>LLM</cell><cell cols="3">ProGen2 (Nijkamp et al., 2023) 61.07±18.45 2.90±0.71</cell><cell>0.43</cell><cell>15.52</cell><cell>0.44</cell><cell>11.02</cell></row><row><cell></cell><cell>ProLLaMA (ours)</cell><cell cols="2">66.49±12.61 3.10±0.65</cell><cell>0.49</cell><cell>9.50</cell><cell>0.48</cell><cell>7.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Controllable generation on four different instructions. Given SAM-MT, TPHD, Trx, and CheY as instructions, ProLLaMA generates proteins with the desired functionalities. High values of TM-score and H-Prob indicate the generated proteins meet the instructions, in contrast to the uncontrollable generation of other models.</figDesc><table><row><cell>Method</cell><cell cols="2">SAM-MT</cell><cell>TPHD</cell><cell></cell><cell>Trx</cell><cell></cell><cell>CheY</cell><cell></cell></row><row><cell></cell><cell cols="8">TM-score↑ H-Prob↑ TM-score↑ H-Prob↑ TM-score↑ H-Prob↑ TM-score↑ H-Prob↑</cell></row><row><cell>ESM-1b</cell><cell>0.58</cell><cell>0.37</cell><cell>0.55</cell><cell>0.48</cell><cell>0.61</cell><cell>0.37</cell><cell>0.63</cell><cell>0.27</cell></row><row><cell>ESM-2</cell><cell>0.52</cell><cell>0.26</cell><cell>0.51</cell><cell>0.25</cell><cell>0.53</cell><cell>0.30</cell><cell>0.57</cell><cell>0.18</cell></row><row><cell>EvoDiff</cell><cell>0.46</cell><cell>1.17</cell><cell>0.42</cell><cell>1.80</cell><cell>0.42</cell><cell>1.10</cell><cell>0.46</cell><cell>1.43</cell></row><row><cell>ProtGPT2</cell><cell>0.45</cell><cell>3.86</cell><cell>0.43</cell><cell>4.62</cell><cell>0.44</cell><cell>2.53</cell><cell>0.45</cell><cell>4.86</cell></row><row><cell>ProGen2</cell><cell>0.44</cell><cell>1.90</cell><cell>0.45</cell><cell>2.49</cell><cell>0.43</cell><cell>2.44</cell><cell>0.44</cell><cell>2.13</cell></row><row><cell>ProLLaMA (ours)</cell><cell>0.71</cell><cell>98.13</cell><cell>0.82</cell><cell>100.00</cell><cell>0.93</cell><cell>99.96</cell><cell>0.81</cell><cell>100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Protein property prediction. Our ProLLaMA achieves nearly 100% accuracy for many superfamilies. Here are 10 of them.</figDesc><table><row><cell>OBFD</cell><cell>UPF0145</cell><cell>NACD</cell><cell>U3S</cell><cell>CCHC</cell><cell>Kazal</cell><cell cols="2">SAM-MT TPHD</cell><cell>Trx</cell><cell>CheY</cell></row><row><cell>Accuracy 100.00%</cell><cell>100.00%</cell><cell cols="4">100.00% 100.00% 95.24% 100.00%</cell><cell>93.67%</cell><cell cols="3">90.84% 94.17% 100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the natural language ability. Vocab represents whether the vocabulary of the model supports natural language. Generation represents the sentence generation.</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell>Vocab</cell><cell>Generation</cell><cell>QA</cell></row><row><cell>GeneralLLM</cell><cell>LLaMA2</cell><cell>✓</cell><cell>45%</cell><cell>44%</cell></row><row><cell></cell><cell>ProGen2</cell><cell>✗</cell><cell>-</cell><cell>-</cell></row><row><cell>ProLLM</cell><cell>ProtGPT2</cell><cell>✓</cell><cell>0%</cell><cell>-</cell></row><row><cell></cell><cell>ProLLaMA</cell><cell>✓</cell><cell>26%</cell><cell>33%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">* Equal contribution 1 School of Electronic and Computer Engineering, Peking University 2 Peng Cheng Laboratory. Correspondence to: Li Yuan &lt;yuanli-ece@pku.edu.cn&gt;, Yonghong Tian &lt;yhtian@pku.edu.cn&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix <ref type="bibr">A.1. Detailed Training Settings</ref> For continual learning, the block size is set to 2048, with a gradient accumulation step count of 8. The warm-up ratio is 0.05, and the weight decay is set to 0.01. The data type is bfloat16. The batch size per GPU is 4. DeepSpeed Zero Redundancy Optimizer stage 2 (ZeRO-2) is employed without using offload. For instruction tuning, the max sequence length is set to 256, with a gradient accumulation step count of 4. The warm-up ratio is 0.03. The batch size per GPU is 144. Other settings are the same as continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Detailed Evaluation Metrics</head><p>We use OmegaFold to calculate the values of the predicted Local Distance Difference Test (pLDDT) of protein sequences. Omegafold performs structure prediction without the need for homologous sequences or evolutionary information, relying solely on a single sequence for prediction. To calculate Perplexity (SC-Perp), following the process mentioned in EvoDiff, we fold the sequence into a 3D structure using OmegaFold, then unfold it back into the sequence using ProteinMPNN. The self-consistency perplexity between the resulting sequence and the original sequence is referred to as SC-Perp.</p><p>We calculate TM-score, RMSD, H-Prob, and Seq-Ident using Foldseek. Foldseek facilitates the pairing of the queried protein p query with structurally similar proteins from an existing protein database (AFDB or PDB), yielding pairs represented as (p query , p target ). Here, p target denotes the protein in the database with a significant structural similarity to p query . The magnitude of the average Template Modeling score (TM-score) value and Root-Mean-Square Deviation (RMSD) reflects the degree of structural similarity. TM-score takes into account the overall topological structure of proteins, focusing more on the overall structure. RMSD calculates the square root of the average position deviation of corresponding atoms between two protein structures, being highly sensitive to the size of the protein structure and local variations. Additionally, Foldseek also calculates the Sequence Identity (Seq-Ident) between p query and p target , reflecting their sequence-level similarity. Homologous probability (H-Prob) reflects the probability that p query and p target is homologous. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7319" to="7328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein generation with evolutionary diffusion: sequence is all you need</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alamdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2029" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the protein language: Evolution, structure, and function</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="654" to="669" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Battistuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Bluhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Iype</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Crystallographica Section D: Biological Crystallography</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="899" to="907" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proteinbert: a universal deep-learning model of protein sequence and function</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2102" to="2110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient and effective text encoding for chinese llama and alpaca</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08177</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The functional importance of structure in unstructured protein regions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Davey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in structural biology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameterefficient fine-tuning of large-scale pre-trained language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Prottrans: Toward understanding the language of life through self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7112" to="7127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Controllable protein design with language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="521" to="532" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Protgpt2 is a deep unsupervised language model for protein design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höcker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4348</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04366</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-W</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01933</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07736</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Kocoń</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cichecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaszyca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szydło</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bielaniewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanclerz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Jack of all trades, master of none. Information Fusion</publisher>
			<biblScope unit="page">101861</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolutionary-scale prediction of atomic-level protein structure with a language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smetanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kabeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="issue">6637</biblScope>
			<biblScope unit="page" from="1123" to="1130" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">P-tuning: Prompt tuning can be comparable to finetuning across scales and tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic instruction optimization for open-source llm instruction tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13246</idno>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large language models generate functional protein sequences across diverse families</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Holton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Olmos</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models enable zero-shot prediction of the effects of mutations on protein function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29287" to="29303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recent advances in natural language processing via large pre-trained language models: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P B</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Design in the dark: learning deep generative models for de novo protein design</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kandathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2022" to="2023" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Progen2: exploring the boundaries of protein language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ruffolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="968" to="978" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tranception: Protein fitness prediction with autoregressive transformers and inference-time retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Notin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/notin22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The language of proteins: Nlp, machine learning &amp; protein sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Structural Biotechnology Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1750" to="1758" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recent advances in de novo protein design: Principles, methods, and applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kortemme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biological Chemistry</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Paysan-Lafosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chuguransky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D418" to="D427" />
			<date type="published" when="2022">2022. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06476</idno>
		<title level="m">Is chatgpt a general-purpose natural language processing task solver? arXiv preprint</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Msa transformer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8844" to="8856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Proximal exploration for model-guided protein sequence design</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/ren22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">e2016239118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Importance weighted expectationmaximization for protein sequence design</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00386</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep generative modeling for protein design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in structural biology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="226" to="236" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding the capabilities, limitations, and societal impact of large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02503</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anyango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Natassia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yordanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laydon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D439" to="D444" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Instructprotein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03269</idno>
		<title level="m">Aligning human and protein language via knowledge instruction</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The roles of mutation, inbreeding, crossbreeding, and selection in evolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Protst: Multimodality learning of protein sequences and biomedical texts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12040</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learned protein embeddings for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Bedbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2642" to="2648" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Evaluating large language models at evaluating instruction following</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Instruction tuning for large language models: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10792</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scoring function for automated assessment of protein structure template quality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structure-informed language models are protein designers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2025" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10198</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Instruction-following evaluation for large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07911</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
