<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-02-26">26 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omkar</forename><surname>Thawakar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashmal</forename><surname>Vayani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hisham</forename><surname>Cholakal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rao</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Akshita</forename><surname>Bha- Gia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Khot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><forename type="middle">Daniel</forename><surname>Merrill</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Soldaini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">2024</forename><surname>Hajishirzi</surname></persName>
						</author>
						<author>
							<persName><surname>Olmo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><roleName>Loubna</roleName><forename type="first">Raymond</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Allal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Terry</forename><forename type="middle">Yue</forename><surname>Zhuo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Dehaene</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mishig</forename><surname>Davaadorj</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">João</forename><surname>Monteiro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Gontier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
						</author>
						<author>
							<persName><roleName>Logesh</roleName><forename type="first">Ming-Ho</forename><surname>Yee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kumar</forename><surname>Umapathi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Lipkin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Muhtasham</forename><surname>Oblokulov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rudra</forename><surname>Murthy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Stillerman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sankalp</forename><surname>Siva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dmitry</forename><surname>Patel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Abulkhanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manan</forename><surname>Zocca</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhihan</forename><surname>Dey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nour</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Urvashi</forename><surname>Fahmy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Bhattacharyya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Swayam</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sasha</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paulo</forename><surname>Luccioni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maxim</forename><surname>Villegas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fedor</forename><surname>Kunakov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Zhdanov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><surname>Romero</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nadav</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Timor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claire</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hailey</forename><surname>Schlesinger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Schoelkopf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Ebert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mayank</forename><surname>Dao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Mishra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Robinson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brendan</forename><surname>Anderson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan- Ish</forename><surname>Dolan-Gavitt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siva</forename><surname>Contractor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Reddy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dzmitry</forename><surname>Fried</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yacine</forename><surname>Bahdanau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><forename type="middle">Muñoz</forename><surname>Jernite</surname></persName>
						</author>
						<author>
							<persName><surname>Ferrandis</surname></persName>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Sean Hughes</orgName>
								<address>
									<addrLine>Thomas Wolf</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Leandro von Werra, and Harm de Vries. 2023a</orgName>
								<orgName type="institution">Arjun Guha</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-26">26 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">82927F935501885C063DC73B81F46E7C</idno>
					<idno type="arXiv">arXiv:2402.16840v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-03-06T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bigger the better' has been the predominant trend in recent Large Language Models (LLMs) development. However, LLMs do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the 'less is more' paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) parameter SLM, named MobiLlama, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. MobiLlama is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. Our work strives to not only bridge the gap in open-source SLMs but also ensures full transparency, where complete training data pipeline, training code, model weights, and over 300 checkpoints along with evaluation codes is available at : https://github.com/ mbzuai-oryx/MobiLlama.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed a tremendous surge in the development of Large Language Models (LLMs) with the emergence of prominent closedsource commercial models such as ChatGPT, Bard, and Claude. These LLMs exhibit surprising capabilities, typically called emergent abilities, towards solving complex tasks. Most existing popular LLMs follow a similar trend that bigger is always better, where scaling model size or data size typically provides improved model capacity and performance on downstream tasks. For instance, * Equal contribution.</p><p>the recent Llama-2 70 billion (70B) model <ref type="bibr" target="#b27">(Touvron et al., 2023)</ref> is considered more favorable in different chat applications due to its effectiveness towards handling dialogues, logical reasoning, coding, compared to its 7B counterpart which is typically better suited for basic tasks such as categorization or summaries. While these LLMs demonstrate impressive performance in handling complex language tasks, a key limitation is their size and computational requirements. For instance, the large-scale Falcon <ref type="bibr" target="#b0">(Almazrouei et al., 2023)</ref> 180B model was trained using 4096 A100 GPUs and requires large memory and compute for deployment with dedicated high-performance servers and scalable storage systems.</p><p>Recently, Small Language Models (SLMs) have shown potential in terms of providing decent performance with emergent abilities achieved at a significantly smaller scale compared to their large-scale LLM counterparts. Modern SLMs like Microsoft's Phi-2 2.7 billion <ref type="bibr" target="#b10">(Li et al., 2023b)</ref> highlight the growing focus in the community on achieving more with less. SLMs offer advantages in terms of efficiency, cost, flexibility, and customizability. With fewer parameters, SLMs offer significant computational efficiency in terms of fast pre-training and inference with reduced memory and storage requirements. This is critical in real-world applications where efficient resource utilization is highly desired. It particularly opens up possibilities in resource-constrained computing, where the models are required to be memory efficient to operate on low-powered devices (e.g., edge). SLMs support on-device processing that enhances privacy, security, response time, and personalization. Such an integration can lead to advanced personal assistants, cloud-independent applications, and improved energy efficiency with a reduced carbon footprint.</p><p>The landscape of language models, especially SLMs, is currently marked by a notable lack of open-source availability. While LLMs have gar-nered significant attention, the proprietary nature of most models has led to limited transparency and accessibility, particularly in the realm of SLMs. This gap hinders the scientific and technological exploration of these more efficient, compact and performant models. Recognizing this, there's a growing need in the community for fully transparent opensource SLMs, which would facilitate a deeper understanding of their capabilities and limitations and spur innovation by allowing broader community access to their architecture and reproducible training methodologies. We argue that bridging this gap is crucial for democratizing access to collaborative advancement for SLMs. Therefore, we investigate the problem of designing accurate yet efficient SLMs from scratch with the intention to provide full transparency in the form of access to entire training data pipeline and code, model weights, more than 300 checkpoints along with evaluation codes.</p><p>When designing a SLM from scratch it is desired that the resulting model is accurate, while maintaining efficiency in terms of pre-training and deployment. A straightforward way is to scaledown a larger LLM design to the desired model size (e.g., 0.5B) by reducing either the size of the hidden dimension layers or the number of layers. We empirically observe both these design strategies to provide inferior performance. This motivates us to look into an alternative way of designing a SLM from scratch that is accurate yet maintains the efficiency, while offering full transparency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We introduce a SLM framework, named MobiLlama, with an aim to develop accurate SLMs by alleviating the redundancy in the transformer blocks. Different to the conventional SLM design where dedicated feed forward layers (FFN) are typically allocated to each transformer block, we propose to employ a shared FFN design for all the transformer blocks within SLM. Our MobiLlama leveraging a shared FFN-based SLM design is accurate and maintains efficiency, while offering full transparency in terms of data pipeline, training code, model weights and extensive intermediate checkpoints along with evaluation codes.</p><p>We empirically show that our MobiLlama performs favorably compared to conventional SLMs design schemes when performing pre-training from scratch. Our MobiLlama 0.5B model outperforms existing SLMs of similar size on nine different benchmarks. MobiLlama 0.5B achieves a gain of 2.4% in terms of average performance on nine benchmarks, compared to the best existing 0.5B SLM in the literature. We further develop a 0.8B SLM that originates from our 0.5B model by utilizing a wider shared-FFN scheme in transformer blocks, achieving top performance among existing SLMs falling under less than 1B parameters category. Lastly, we build multimodal models on top of our SLM to showcase visual perception and reasoning capabilities. Fig. <ref type="figure" target="#fig_0">1</ref> shows a comparison of our MobiLlama with recent fully transparent relatively larger SLMs in terms of accuracy, pre-training complexity and on-board deployment cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While LLMs have gained tremendous popularity <ref type="bibr">(Zhao et al., 2023)</ref>, one of their key limitations is the size and computational requirements both during pre-training and deployment. Another issue is limited availability of fully transparent openssource LLMs that provide complete access to data pipeline, training code along with checkpoints and evaluation protocols. Prior works explore making several components of LLM framework efficient such as, attention mechanism <ref type="bibr" target="#b5">(Dao, 2023)</ref> and optimization strategies <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2017)</ref>. Further, existing efforts also include exploring posttraining sparsification schemes <ref type="bibr" target="#b0">(Ashkboos et al., 2024)</ref> or quantization <ref type="bibr" target="#b0">(Hoefler et al., 2021;</ref><ref type="bibr" target="#b36">Zhu et al., 2023;</ref><ref type="bibr" target="#b29">Xiao et al., 2023)</ref>  Recently, designing SLMs from scratch have gained attention <ref type="bibr" target="#b1">(Biderman et al., 2023;</ref><ref type="bibr" target="#b28">Wu et al., 2023;</ref><ref type="bibr" target="#b32">Zhang et al., 2024;</ref><ref type="bibr" target="#b13">Li et al., 2023a;</ref><ref type="bibr" target="#b12">Lin et al., 2021b;</ref><ref type="bibr" target="#b24">Shoeybi et al., 2019;</ref><ref type="bibr" target="#b33">Zhang et al., 2022)</ref>. SLMs have shown potential as an alternative especially in case of limited pre-training compute as well as deployment in resource-constrained environments (e.g., edge devices). Further, SLMs can support on-device processing which in turn can enhance security, privacy, response efficiency, and personalization. Here, we strive to construct fully transparent accurate yet computationally efficient SLMs by maintaining the model's capacity to capture complex patterns and relationships in data while reducing the redundancy often present in the parameters of SLMs. Prior works <ref type="bibr" target="#b7">(Frantar et al., 2022;</ref><ref type="bibr">Gholami et al., 2022;</ref><ref type="bibr" target="#b21">Pires et al., 2023;</ref><ref type="bibr" target="#b19">Pan et al., 2023;</ref><ref type="bibr">Bhojanapalli et al., 2021)</ref> exploring alleviating redundancy in transformer design either focusing on the attention mechanism or on the single feed-forward layer in BERT style architectures. Different from these approaches, we explore alleviating the redundancy in the SLM architectures with an LLM objective function by focusing on the sharing mechanism of MLP blocks having multiple feed-forward network (FFN) layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline SLM Design</head><p>We first describe our baseline 0.5B SLM architecture that is adapted from recent TinyLlama <ref type="bibr" target="#b32">(Zhang et al., 2024)</ref> and Llama-2 <ref type="bibr" target="#b27">(Touvron et al., 2023)</ref>. The baseline architecture comprises N layers, where each layer consists of hidden dimensions of M and intermediate size (MLPs) of 5632. The vocabulary size is 32K and max. context length is C. We consider two different design choices when constructing a 0.5B model from scratch. In first design choice, named baseline1, the number of layer is set to N = 22 and hidden size of each layer is set to M = 1024. In second design choice, named baseline2, we set the number of layer to N = 8 and hidden size of each layer is set to M = 2048.</p><p>We note that both the aforementioned baseline designs struggle to strike an optimal balance between accuracy and efficiency. While a reduced size of hidden dimensions (1024) in case of baseline1 aids in computational efficiency, it can likely hamper the model's capacity to capture complex patterns within the data. Such a reduction in dimension can potentially lead to a bottleneck effect, where the model's ability to represent intricate relationships and nuances in the data is constrained, thereby affecting the overall accuracy. On the other hand, reducing the number of hidden layers (22 to 8), as in the baseline2, affects the model's depth that in turn hampers its ability to learn hierarchical representations of the language. Achieving superior performance on tasks requiring deeper linguistic comprehension and contextual analysis likely requires combining the advantages of the two aforementioned baselines. However, increasing the model capacity of baseline1 and baseline2 into a single model (22 layers and hidden dimension size of 2048) results in a significantly larger parameterized model of 1.2B with increased training cost (see Tab. 1). We name this larger model as largebase. Next, we present our proposed MobiLlama 0.5B model design that does not reduce hidden dimension size in each layer (baseline1) or the total number of layers (baseline2), while maintaining a comparable training efficiency (see Tab. 1). For each case, we show two transformer blocks denoted by different self-attention layers. In the case of both baseline1 and baseline2, a dedicated MLP block comprising three FFN layers is utilized for each transformer layer. In contrast, our MobiLlama utilizes a single MLP block (highlighted by the same color) that is shared across different transformer layers. This enables to increase the capacity of the network in terms of layers and hidden dimension size without any significant increase in the total number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed SLM Design: MobiLlama</head><p>The proposed approach, MobiLlama, constructs a SLM of desired sizes (e.g., 0.5B model) by first initiating from a larger model size design, largebase. Then, we employ a careful parameter sharing scheme to reduce the model size to a pre-defined model configuration, thereby significantly reducing the training cost. Generally, both SLMs and LLMs typically utilize a dedicated multilayer perceptron (MLP) block comprising multiple feed forward network (FFN) layers within each transformer block. In such a configuration (e.g., large-base), the FFN layers account for a substantial 65% of the total trainable parameters, with attention mechanisms and heads contributing 30% and 5%, respectively. As a consequence, a significant number of parameters are concentrated within the FFN layers, thereby posing challenges during pre-training with respect to computational cost and the model's ability to achieve faster convergence. To address these issues, we propose to use a sharing scheme where the FFN parameters are shared across all transformer layers within the SLM. This enables us to significantly reduce the overall trainable parameters by 60% in our MobiLlama, compared to the large-base. Such a significant parameter reduction also enables us to increase the model capacity in terms of number of layers and hidden dimension size without any substantial increase in the training cost (see Tab. 1).</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> compares our architecture design with two baselines. In case of both baselines, a dedicated MLP block that consists of multiple FFN layers is used in each transformer layer. Instead, our ef-ficient MobiLlama design utilizes a single MLP block which is shared across different layers of transformer within the SLM. This helps in increasing the model capacity without any increase in the total number of trainable parameters in the model.   <ref type="bibr" target="#b14">(Liu et al., 2023b)</ref>. The Amber dataset provides a rich and varied linguistic landscape having different text types, topics, and styles. Tab. 2 shows the data mix from Amber dataset gathered from various sources. Arxiv (30 Billion Tokens) subset is drawn from the repository of scientific papers, provides complex, domain-specific language and technical terminology, enriching the understanding of academic prose. Book (28.9 Billion Tokens) subset comprises tokens from a broad range of literature with diverse narrative styles, cultural contexts, and rich vocabulary, deepening the grasp of storytelling and language nuances. C4 (197.7 Billion Tokens) is the Colossal Clean Crawled Corpus (C4) that offers a vast and cleaned selection of web text, providing a broad linguistic foundation that includes various registers, styles, and topics. Refined-Web (665 Billion Tokens) subset is a curated web crawl and offers the model exposure to contemporary, informal, and varied internet language, enhancing the relevance and applicability to modern communication. StarCoder (291.9 Billion Tokens) subset is a vast collection used for code understanding featuring 783GB of code across 86 programming languages. It includes GitHub issues, Jupyter notebooks, and commits, totaling approximately 250 billion tokens. These are meticulously cleaned and de-duplicated for training efficiency. StackExchange (21.8 Bil-lion Tokens) is from the network of Q&amp;A websites, this subset aids the model in learning questionanswering formats and technical discussions across diverse topics. Wikipedia (23.9 Billion Tokens) is an encyclopedia collection, it offers well-structured and factual content that helps the model to learn encyclopedic knowledge and formal writing styles.</p><p>From the above-mentioned subsets, Arxiv, Book, C4, StackExchange and Wikipedia are sourced from <ref type="bibr">RedPajama-v1 (Computer, 2023)</ref>. The Amber dataset uses RefinedWeb <ref type="bibr" target="#b20">(Penedo et al., 2023)</ref> data to replace common_crawl subset of RedPajama-v1. These subsets amount to 1259.13 billion tokens.</p><p>Initially, raw data sourced from the above sources is tokenized using Huggingface LLaMA tokenizer <ref type="bibr" target="#b27">(Touvron et al., 2023)</ref>. Subsequently, these tokens are organized into sequences with each containing 2048 tokens. To manage data, these sequences are merged to the token sequences and divided the amalgamated dataset into 360 distinct segments. Each data segment, structured as a jsonl file, carries an array of token IDs along with a source identifier that denotes the originating dataset. Each data sample is designed to have 2049 tokens. Architecture Design: Our MobiLlama 0.5B comprises a hidden size of 2048, an intermediate size of 5632 in its MLPs, and operates with 32 attention heads across 22 hidden layers. It is designed to handle sequences up to 2048 tokens long, supported by a vocabulary size of 32,000. The precision in normalization is ensured by an RMSNorm epsilon of 1e −6 to obtain a more stable training. We utilize RoPE (Rotary Positional Embedding) <ref type="bibr" target="#b26">(Su et al., 2024)</ref> to encode positional information in our MobiLlama. Similar to <ref type="bibr" target="#b32">(Zhang et al., 2024)</ref>, we employ a combination of Swish and Gated Lin-   <ref type="bibr" target="#b27">(Touvron et al., 2023)</ref> model. Our MobiLlama model's training is performed using the AdamW optimizer, leveraging hyperparameters β 1 = 0.9, β 2 = 0.95, with an initial learning rate of η = 3e −4 . This rate follows a cosine learning rate schedule, tapering to a final rate of η = 3e −5 . We further incorporate a weight decay of 0.1 and apply gradient clipping at 1.0 with a warm-up period over 2, 000 steps. Adapting to our hardware configuration of 20 GPU nodes, we optimize the pre-training batch size to 800 (160 × 5), achieving a throughput of approximately 14k-15k tokens per second on a single GPU. During our model pretraining, we save intermediate checkpoints after every 3.3B tokens which will be publicly released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Benchmarks and Metrics:</head><p>For a comprehensive performance evaluation, we use nine different benchmarks from the Open LLM Leaderboard 1 .</p><p>HellaSwag <ref type="bibr" target="#b31">(Zellers et al., 2019)</ref> assesses the model's ability to predict the correct ending to a scenario from a set of possible continuations, Following the Analysis-360 framework <ref type="bibr" target="#b14">(Liu et al., 2023b)</ref> that is built on llm-harness <ref type="bibr" target="#b1">(Gao et al., 2023)</ref>, we conduct extensive evaluations under the standard settings with varying shots for detailed assessments, validating the model's robustness and adaptability across diverse linguistic tasks. Following the standard evaluation protocol, our evaluation setting consists of 10, 25, 5 and 5 shot evaluation for Hellaswag, ARC_Challenge, Winogrande and MMLU, while zero-shot for rest of the benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Baseline Comparison: We first present a comparison with the two baselines in Tab. 5) for 0.5B model series. For the baseline evaluation, we pretrain all the models on the same 100B tokens from the Amber dataset and report the results on four benchmarks: HellaSwag, TruthfulQA, MMLU, and Arc_C. Our MobiLlama achieves favourable performance compared to the two baselines by achieving an average score of 34.4 over the four benchmarks. We note that this performance improvement is achieved without any significant increase in the training cost (see Tab. 1), highlighting the merits of the proposed SLM design. State-of-the-art Comparison: We compare our MobiLlama 0.5B and 0.8B with existing SLMs having comparable (less than 1B) parameters: gpt-  neo <ref type="bibr" target="#b2">(Black et al., 2021)</ref>, tiny-starcoder <ref type="bibr" target="#b13">(Li et al., 2023a)</ref>, cerebras-gpt <ref type="bibr" target="#b6">(Dey et al., 2023)</ref>, opt <ref type="bibr" target="#b33">(Zhang et al., 2022)</ref>, megatron-gpt-2 <ref type="bibr" target="#b24">(Shoeybi et al., 2019)</ref>, LiteLlama, gpt-sw3, pythia <ref type="bibr" target="#b1">(Biderman et al., 2023)</ref>, xglm <ref type="bibr" target="#b12">(Lin et al., 2021b)</ref>, Lamini-LM <ref type="bibr" target="#b28">(Wu et al., 2023)</ref>. Among existing methods falling around 0.5B model series category, pythia-410m achieves an average score of 43.57. Our MobiLlama 0.5B model achieves superior performance with an average score of 46.0, outperforming pythia-410m by 2.4% in terms of average performance on nine benchmarks. Notably, MobiLlama achieves superior performance on the HellaSwag benchmark which is designed to evaluate the model's capabilities in the NLP text completion task. Further, Mo-biLlama also performs favorably on commonsense reasoning tasks with superior results on piqa and winogrande benchmarks. Table <ref type="table">8</ref>: Quantitative performance of our multimodal design, MobiLlama-V 0.8B, on different benchmarks.</p><p>source consumption on various low-end hardware platforms: a PC with RTX-2080Ti GPU, a laptop with i7 CPU, and a smartphone with Snapdragon-685 processor. Tab. 6 shows the comparison of our MobiLlama 0.5B with large-base 1.2B, Llama2-7B <ref type="bibr" target="#b27">(Touvron et al., 2023)</ref> and Phi2-2.7B <ref type="bibr" target="#b10">(Li et al., 2023b)</ref> model, in terms of the average processing speed in tokens per second (Average Tokens/Sec), average memory consumption (Avg Memory Consumption) in megabytes (MB), and the average battery consumption (Average Battery Consumption/1000 Tokens) in milliampere-hours (mAH).</p><p>Our MobiLlama performs favorably in terms of efficiency across different hardware platforms.</p><p>We further perform an efficiency comparison to a recent post-training sparsification scheme <ref type="bibr" target="#b0">(Ashkboos et al., 2024)</ref>, where each weight matrix is substituted with a smaller (dense) matrix, thereby reducing dimensions of the embeddings in the model. In such a scheme, the parameters of the original LLM are reduced significantly up to 70% followed by post-slicing fine-tuning using a dataset such as WikiText-2 <ref type="bibr" target="#b17">(Merity et al., 2016)</ref>. Tab. 7 shows the comparison of our MobiLlama with existing LLMs (e.g., Llama-2-7B, OPT-6.7B) on four benchmarks following <ref type="bibr" target="#b0">(Ashkboos et al., 2024)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Human : Describe this image in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobiLlama :</head><p>The image depicts a group of people walking through a rocky, hilly terrain, likely on a mountain trail. There are at least 13 people in the scene, with some of them carrying backpacks. The group is spread out along the trail, with some people walking closer to the camera and others further in the background. The people appear to be enjoying their walk, possibly exploring the area or taking a break from their journey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Human : What does the provided image contain ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobiLlama :</head><p>The image shows a tray with a variety of food items, including a sandwich, fruit, vegetables, and a sauce. There are also napkins and a plate with a small meal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Human : What are people in image doing ?</p><p>MobiLlama :</p><p>These people in the image are performing a trick where they are in mid-air, balancing on their toes. They are all jumping together, showcasing their athleticism and skill in balancing and performing tricks. They are all in the air, with their feet hanging off the ground, demonstrating their ability to maintain balance and control during the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Human : Write a suitable poem for this scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobiLlama :</head><p>As the night falls, The bridge stands tall, A medieval wonder, A sight to behold, In the midst of the forest, A bridge that holds the heart, Of those who pass by, And those who walk on it, A bridge that transcends, The world, and the time.</p><p>Human : How to prepare the food shown in an image ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MobiLLama :</head><p>To make this food, you would need to prepare a sandwich, which typically involves spreading a sauce on ….. lama 0.5B and 0.8B models perform favorably against representative LLMs, with an average score of 53.72 computed over four benchmarks. These results highlight the potential of designing new fully transparent SLMs that can achieve comparable capabilities of their larger sliced model counterparts.</p><p>Multimodal MobiLlama: We further build a multimodal model on top of our MobiLlama by combining it with a vision encoder to develop a generalpurpose visual assistant having visual reasoning capabilities. Our multimodal model, MobiLlama-V , is trained by bridging the visual encoder of CLIP <ref type="bibr" target="#b21">(Radford et al., 2021)</ref> with the language decoder of our MobiLlama, and fine-tuning it in an end-to-end fashion on a 665k vision-language instruction set <ref type="bibr" target="#b13">(Liu et al., 2023a)</ref>. We conduct evaluation on GQA (Hudson and Manning, 2019), SQA <ref type="bibr" target="#b16">(Lu et al., 2022)</ref>, TextQA <ref type="bibr" target="#b25">(Singh et al., 2019)</ref>, and MME <ref type="bibr" target="#b8">(Fu et al., 2023)</ref>. Tab. 8 shows the performance of MobiLlama-V 0.8B model.</p><p>Qualitative Analysis: Fig. <ref type="figure" target="#fig_4">3</ref> shows example responses obtained when interacting with MobiLlama 0.5B with conversation capabilities. We show examples covering different tasks such as, text completion, code generation and conversation capabilities. Our model generates faithful responses to these diverse interactions. Fig. <ref type="figure" target="#fig_5">4</ref> shows examples demonstrating visual reasoning capabilities of our multimodal MobiLlama-V . For instance, MobiLlama-V accurately describes the atypical aspects of the image when asked to describe the given image.</p><p>Evaluating Large-base Model: As discussed ear-lier, we strive to develop fully transparent models for democratization of SLMs and fostering future research. To this end, we compare our large-base 1.2B with existing fully transparent SLMs falling within the less than 2B category. Tab. 9 shows that compared to recent OLMo and TinyLlama that are pre-trained on a larger dataset of 3T tokens, our large-base 1.2B model pre-trained on 1.2T tokens achieves favourable results with an average score of 49.06 over nine benchmarks. We hope that our large-base model will serve as a solid baseline and help ease future research in SLM development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a fully transparent SLM, MobiLlama, that alleviates redundancy in the transformer block. Within MobiLlama, we propose to utilize a shared FFN design for all the blocks within the SLM. We evaluate MobiLlama on nine benchmarks, achieving favourable results compared to existing methods falling under less than 1B category. We also build a multimodal model on top of MobiLlama SLM to demonstrate visual reasoning capabilities. Limitation and Future Direction: A potential direction is to further improve MobiLlama for enhanced context comprehension. While MobiLlama offers a fully transparent SLM framework, a followup study to understand any misrepresentations and biases is desired to improve model's robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Story Creation:</head><p>Human: Create the opening paragraph of a suspenseful thriller: In the dimly lit alley, Detective Miller knew something was off. The air was thick with tension, and as he turned the corner, he discovered a hidden door leading to a world of MobiLlama-0.8B: icy secrets that would soon be revealed in his own life." "The first thing I noticed about this book is how well-written it really felt like an actual story from beginning to end," said one reviewer on Goodreads who also gave her rating for "10 out of ten stars". Another reviewed by another author wrote:"This novel has been described many times over but never quite captured my attention until now...I'm not sure if you can put your finger on what makes me love these books so much or maybe just because they are such good reads! They have all sorts of twists along their way which keeps them fresh every time!"</p><p>Technological Evolution:</p><p>Human: Once people considered Artificial Intelligence is something impossible to achieve, but now MobiLlama-0.8B:</p><p>The future of artificial intelligence: AI will be used in every aspect from medicine and healthcare through automation for manufacturing industries like robotics or even autonomous driving systems that can drive themselves without human intervention (or at least with less risk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytical Thinking:</head><p>Human: Create a concept for an album cover that represents the concept of time travel through music MobiLlama-0.8B: and art, or something similar to this: Time Travel Through Music -A song with lyrics about how you can go back in history if only one person would listen. . . or maybe it's just me! I don't know what my favorite is yet but there are so many songs out now on YouTube where people talk about going backwards into their past lives (like "Time Machine" by The Beatles) which makes sense because they were talking about getting ridiculous amounts of money from someone who was dead when we get here today; however some artists have gone further than simply saying "I want all your stuff" like Kanye West did last year...he actually went as far as creating his own musical instrument called 'The Tidal Wave'. It basically looks exactly like he does at times during his live shows except instead of being able to see himself perform inside another body while singing along, users will be transported directly onto stage via wave technology using headphones attached to them..and then afterward everyone gets together again once more!! This could also mean anything really though since most musicians do not even realize themselves performing outside of reality anymore anyway....but whatever works best for YOU!!!</p><p>Programming and Logic Building:</p><p>Human: Generate a Python code snippet that implements a function to calculate the Fibonacci sequence using recursion. Bonus points if you can make it more efficient using memoization.</p><p>MobiLlama-1.2B: Sure, here's a Python code snippet that implements a function to calculate the Fibonacci sequence using recursion: "' def fibonacci(n): if n &lt;= 1: return n else:</p><p>return fibonacci(n-1) + fibonacci(n-2)</p><p>To make this function more efficient using memoization, we can store the previous values of the Fibonacci sequence in a dictionary. Here's an updated version of the function that uses memoization: This function takes a number n as input and returns the nth Fibonacci number, or the value n if it's already been calculated. If n is less than or equal to 1, the function returns the value n. Otherwise, it recursively calls itself with the previous two numbers in the sequence and adds them together. If the value of n is already in the dictionary, the function returns the value of n directly. Otherwise, it adds the value of n to the dictionary and returns the value of n with the memoized value. This updated function is more efficient because it avoids calculating the same value multiple times, which can be a bottleneck in recursive functions.</p><formula xml:id="formula_0">def</formula><p>Innovative Thinking:</p><p>Human: Construct a sentence that uses every letter of the alphabet at least once.</p><p>MobiLlama-1.2B: I am just a 10-year-old's birthday party planner. I love organizing and planning fun and memorable events for kids. I am always up for a challenge and love to create unique and exciting experiences for my clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creativity and Imagination:</head><p>Human: Write a role-play with a tourist and a travel guide for a finctional planet.</p><p>MobiLlama-1.2B: Sure, here's a role-play: ### Tourist: Hi, I'm a tourist from Earth. I'm here to explore the finctional planet. Can you tell me more about it? ### Travel Guide: Sure, the finctional planet is a unique and fascinating place. It's a world where science and magic collide, where the laws of physics are different from our own. ### Tourist: That sounds amazing! Can you tell me more about the planet's history and culture? ### Travel Guide: Sure, the planet was created by a group of scientists who wanted to explore the limits of human knowledge. They created a simulation to test their theories, and the simulation went wrong. The simulation created a world where science and magic intertwined, and the planet became a place where people could live and work together. ### Tourist: That sounds amazing! Can you tell me more about the planet's technology and infrastructure? ### Travel Guide: Sure, the planet has advanced technology that allows people to travel through time and space. It also has a network of underground tunnels that connect different parts of the planet. ### Tourist: That sounds amazing! Can you tell me more about the planet's natural resources and resources? ### Travel Guide: Sure, the planet has a vast array of natural resources that are essential for the planet's survival. It has a unique ecosystem that is home to a variety of species, and it has a thriving economy that relies on the planet's resources. ### Tourist: That sounds amazing! Can you tell me more about the planet's history and culture? ### Travel Guide: Sure, the planet has a rich history that dates back to the beginning of time. It has a complex society that is based on a balance between science and magic. ### Tourist: That sounds amazing! Can you tell me more about the planet's future? ### Travel Guide: Sure, the planet is in the process of creating a new civilization that will be based on a fusion of science and magic. It will be a place where people can live and work together, and it will be a place where science and magic will be in harmony. ### Tourist: That sounds amazing! Can you tell me more about the planet's future? ### Travel Guide: Sure, the planet is in the process of creating a new civilization that will be based on a fusion of science and magic. It will be a place where people can live and work together, and it will be a place where science and magic will be in harmony.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of our MobiLlama 0.5B and 0.8B models with recent OLMo-1.17B (Groeneveld et al., 2024) and TinyLlama-1.1B (Zhang et al., 2024) in terms of pre-training tokens, pre-training time and memory, model parameters, overall accuracy across nine benchmarks and on-device efficiency (average battery consumption and average token/second on a PC with RTX2080Ti). Our MobiLlama achieves comparable accuracy while requiring significantly fewer pre-training data (1.2T tokens vs. 3T tokens), lesser pre-training time and GPU memory along with being efficient in terms of deployment on a resource constrained device.</figDesc><graphic coords="2,309.14,72.11,209.54,145.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrative comparison of our MobiLlama with the two baselines.For each case, we show two transformer blocks denoted by different self-attention layers. In the case of both baseline1 and baseline2, a dedicated MLP block comprising three FFN layers is utilized for each transformer layer. In contrast, our MobiLlama utilizes a single MLP block (highlighted by the same color) that is shared across different transformer layers. This enables to increase the capacity of the network in terms of layers and hidden dimension size without any significant increase in the total number of trainable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Further, our MobiLlama 0.8B model achieves an average score of 49.06. Efficiency Comparison: We present the comparison of our model in terms of efficiency and re-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 9 :</head><label>9</label><figDesc>Comprehensive comparisons with existing &lt; 2B params fully open-source LLM models on 9 benchmarks. Our 1.2B large-base model pre-trained on 1.2T tokens achieves superior performance compared to both the recent OLMo 1.17B model(Groeneveld et al., 2024)  and TinyLlama 1.1B model<ref type="bibr" target="#b32">(Zhang et al., 2024)</ref>, which are pre-trained on a substantially larger data of 3T tokens.On a bright sunny day ... 10 years ago, I was standing in the middle of my backyard with an empty beer bottle and some old magazines. I had just finished reading "The Artist's Way" by Julia Cameron when she said something that stuck to me for many months afterward: The artist is not always right; he or her may even have been wrong at one time but has since changed his mind about ..... Write a Python function that calculates the factorial of a given number. Sure, here's a Python function that calculates the factorial of a given number: ```python def factorial(n): if n == 0: return 1 else: return n * factorial(n-1) ```E xplain the economic factors contributing to the rise or fall of a country's currency. Some of the factors that contribute to the rise or fall of a country's currency includes : 1. Interest rates: The interest rates in a country's currency can affect ……. 2. Exchange rates: The exchange rates between the currency of a ….. 3. Central Bank policies: The policies of the central ….. 4. Political instability: Political instability can …… Write steps on how to cook a pizza? Sure! Here are the steps: 1. Preheat the oven to 400°F (200°C) and line a baking sheet with parchment paper.2. Spread the pizza dough with a thin layer of tomato sauce, mozzarella cheese, and basil.3. Bake the pizza for 10-12 minutes, or until the cheese is melted and the crust is golden brown. ……..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example responses from our MobiLlama across a variety of tasks, including creative storytelling, coding exercises, economic analysis, and cooking instructions. The responses highlight the models' ability to engage with both abstract concepts and practical, step-by-step processes, demonstrating its broad applicability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example responses of MobiLlama-V in responding to visual stimuli across a range of scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our MobiLlama with the two baselines and the large-base model. We show the comparison in terms of total number of parameters, training time, total GPU hours, GPU memory, number of transformer layers and the hidden dimension size in each layer. The numbers are computed on A100 GPUs with 80 GB memory each.</figDesc><table><row><cell>of computationally</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Data mix in Amber-Dataset.</figDesc><table><row><cell>Subset</cell><cell cols="2">Tokens (Billion)</cell></row><row><cell>Arxiv</cell><cell>30.00</cell></row><row><cell>Book</cell><cell>28.86</cell></row><row><cell>C4</cell><cell cols="2">197.67</cell></row><row><cell>Refined-Web</cell><cell cols="2">665.01</cell></row><row><cell>StarCoder</cell><cell cols="2">291.92</cell></row><row><cell>StackExchange</cell><cell>21.75</cell></row><row><cell>Wikipedia</cell><cell>23.90</cell></row><row><cell>Total</cell><cell cols="2">1259.13</cell></row><row><cell>Hyperparameter</cell><cell></cell><cell>Value</cell></row><row><cell>Number Parameters</cell><cell></cell><cell>0.5B</cell></row><row><cell>Hidden Size</cell><cell></cell><cell>2048</cell></row><row><cell cols="3">Intermediate Size (in MLPs) 5632</cell></row><row><cell cols="2">Number of Attention Heads</cell><cell>32</cell></row><row><cell cols="2">Number of Hidden Layers</cell><cell>22</cell></row><row><cell>RMSNorm ϵ</cell><cell></cell><cell>1e −6</cell></row><row><cell>Max Seq Length</cell><cell></cell><cell>2048</cell></row><row><cell>Vocab Size</cell><cell></cell><cell>32000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MobiLlama architecture &amp; hyperparameters.</figDesc><table><row><cell>Model Name</cell><cell cols="8">#Params HellaSwag Truthfulqa MMLU Arc_C CrowsPairs piqa race siqa winogrande Average</cell></row><row><cell>gpt-neo-125m</cell><cell>0.15B</cell><cell>30.26</cell><cell>45.58</cell><cell>25.97 22.95</cell><cell>61.55</cell><cell>62.46 27.56 40.33</cell><cell>51.78</cell><cell>40.93</cell></row><row><cell>tiny-starcoder</cell><cell>0.17B</cell><cell>28.17</cell><cell>47.68</cell><cell>26.79 20.99</cell><cell>49.68</cell><cell>52.55 25.45 38.28</cell><cell>51.22</cell><cell>37.86</cell></row><row><cell>cerebras-gpt-256m</cell><cell>0.26B</cell><cell>28.99</cell><cell>45.98</cell><cell>26.83 22.01</cell><cell>60.52</cell><cell>61.42 27.46 40.53</cell><cell>52.49</cell><cell>40.69</cell></row><row><cell>opt-350m</cell><cell>0.35b</cell><cell>36.73</cell><cell>40.83</cell><cell>26.02 23.55</cell><cell>64.12</cell><cell>64.74 29.85 41.55</cell><cell>52.64</cell><cell>42.22</cell></row><row><cell cols="2">megatron-gpt2-345m 0.38B</cell><cell>39.18</cell><cell>41.51</cell><cell>24.32 24.23</cell><cell>64.82</cell><cell>66.87 31.19 40.28</cell><cell>52.96</cell><cell>42.81</cell></row><row><cell>LiteLlama</cell><cell>0.46B</cell><cell>38.47</cell><cell>41.59</cell><cell>26.17 24.91</cell><cell>62.90</cell><cell>67.73 28.42 40.27</cell><cell>49.88</cell><cell>42.26</cell></row><row><cell>gpt-sw3-356m</cell><cell>0.47B</cell><cell>37.05</cell><cell>42.55</cell><cell>25.93 23.63</cell><cell>61.59</cell><cell>64.85 32.15 41.56</cell><cell>53.04</cell><cell>42.48</cell></row><row><cell>pythia-410m</cell><cell>0.51B</cell><cell>40.85</cell><cell>41.22</cell><cell>27.25 26.19</cell><cell>64.20</cell><cell>67.19 30.71 41.40</cell><cell>53.12</cell><cell>43.57</cell></row><row><cell>xglm-564m</cell><cell>0.56B</cell><cell>34.64</cell><cell>40.43</cell><cell>25.18 24.57</cell><cell>62.25</cell><cell>64.85 29.28 42.68</cell><cell>53.03</cell><cell>41.87</cell></row><row><cell>Lamini-GPT-LM</cell><cell>0.59B</cell><cell>31.55</cell><cell>40.72</cell><cell>25.53 24.23</cell><cell>63.09</cell><cell>63.87 29.95 40.78</cell><cell>47.75</cell><cell>40.83</cell></row><row><cell>MobiLlama (Ours)</cell><cell>0.5B</cell><cell>52.52</cell><cell>38.05</cell><cell>26.45 29.52</cell><cell>64.03</cell><cell>72.03 33.68 40.22</cell><cell>57.53</cell><cell>46.00</cell></row><row><cell>Lamini-GPT-LM</cell><cell>0.77B</cell><cell>43.83</cell><cell>40.25</cell><cell>26.24 27.55</cell><cell>66.12</cell><cell>69.31 37.12 42.47</cell><cell>56.59</cell><cell>45.49</cell></row><row><cell>MobiLlama (Ours)</cell><cell>0.8B</cell><cell>54.09</cell><cell>38.48</cell><cell>26.92 30.20</cell><cell>64.82</cell><cell>73.17 33.37 41.60</cell><cell>57.45</cell><cell>46.67</cell></row></table><note>3.3 Towards Fully Transparent MobiLlamaAs discussed earlier, fully transparent open-source SLM development is desired to foster a more inclusive, data/model provenance, and reproducible collaborative SLM research development environment. To this end, we present here pre-training dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>State-of-the-art comparisons with existing &lt; 1B params models on nine benchmarks. In case of around 0.5B model series, our MobiLlama achieves a substantial gain of 2.4% in terms of average performance on nine benchmarks. Further, our MobiLlama 0.8B model achieves an average score of 46.67.</figDesc><table /><note>and processing details, architecture design configuration with training details, evaluation benchmarks and metrics. In addition, we will publicly release complete training and evaluation codes along with intermediate model checkpoints. Pre-training Dataset and Processing: For pretraining, we use 1.2T tokens from LLM360 Amber dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Baseline comparison on four benchmarks. Here, both the baselines and our MobiLlama comprise the same parameters (0.5B) and are pre-trained on 100B tokens from Amber. Our MobiLlama achieves favorable performance compared to the two baselines, while operating on a similar training budget.ear Units together as activation functions. Tab. 3 presents details of our model configuration. We also derive a 0.8B version from our MobiLlama by widening the shared FFN design. Compared to the 0.5B model, our 0.8B design increases the hidden dimension size to 2532 and the intermediate size to 11,080 while the rest of the configuration is same.For pre-training of our MobiLlama, we use a public cluster having 20 GPU nodes each equipped with 8 NVIDIA A100 GPUs with 80 GB memory each and 800 Gbps interconnect for model training. Each GPU is interconnected through 8 NVLink links, complemented by a cross-node connection configuration of 2 port 200 Gb/sec (4× HDR) InfiniBand, optimizing the model's training process. To further enhance the training efficiency, we employ flash-attention mechanism and follow the pre-training hyper-parameters established by the LLaMA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison in terms of efficiency and resource consumption on different low-end hardware devices. We show the comparison on: a PC with RTX-2080Ti GPU, a laptop with i7 CPU and a smartphone with Snapdragon-685 processor. In addition to our large-base model, we also present the comparison with Llama2 7B and Phi2 2.7B. In case of CPU and smartphone, we use 4-bit GGUF format of the corresponding models, whereas the original models are deployed and tested on PC with RTX-2080Ti GPU. The different metrics measure the model's operational efficiency, model's footprint in the device's RAM and the energy efficiency of processing 1,000 tokens. Our MobiLlama performs favorably in terms of efficiency on these low-end hardware devices. We note that both Phi2 and Llama2 are not fully transparent in that the complete data pipeline for pre-training is not publicly available.</figDesc><table><row><cell>Model</cell><cell>#Slice #Params HellaS Arc_C piqa wino Average</cell></row><row><cell>OPT-1.3B</cell><cell>30% 0.91B 39.81 25.77 60.77 54.7 45.26</cell></row><row><cell>OPT-6.7B</cell><cell>30% 4.69B 54.56 29.01 68.61 60.69 53.21</cell></row><row><cell cols="2">Llama-2-7B 30% 4.9B 49.62 31.23 63.55 61.33 51.43</cell></row><row><cell>Phi2-2.7B</cell><cell>30% 1.89B 47.56 30.29 65.94 63.14 51.73</cell></row><row><cell cols="2">Dense 0.5B 52.52 29.52 72.03 57.53 52.90 MobiLlama Dense 0.8B 54.09 30.20 73.17 57.45 53.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison on 4 open LLM benchmarks when parameters are sliced down to 30% using Wiki2Text dataset, following<ref type="bibr" target="#b0">(Ashkboos et al., 2024)</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>. Our MobiL-</figDesc><table><row><cell>Model</cell><cell cols="8">#Params HellaSwag Truthfulqa MMLU Arc_C CrowsPairs piqa race siqa winogrande Average</cell></row><row><cell>Boomer</cell><cell>1B</cell><cell>31.62</cell><cell>39.42</cell><cell>25.42 22.26</cell><cell>61.26</cell><cell>57.99 28.99 40.32</cell><cell>50.98</cell><cell>39.80</cell></row><row><cell>Pythia-Dedup</cell><cell>1B</cell><cell>49.63</cell><cell>38.92</cell><cell>24.29 29.09</cell><cell>67.11</cell><cell>70.23 32.44 42.63</cell><cell>53.98</cell><cell>45.36</cell></row><row><cell>Falcon-RW</cell><cell>1B</cell><cell>63.12</cell><cell>35.96</cell><cell>25.36 35.06</cell><cell>69.04</cell><cell>74.10 36.07 40.23</cell><cell>61.88</cell><cell>48.98</cell></row><row><cell>TinyLlama</cell><cell>1.1B</cell><cell>60.22</cell><cell>37.59</cell><cell>26.11 33.61</cell><cell>70.60</cell><cell>73.28 36.45 41.65</cell><cell>59.18</cell><cell>48.74</cell></row><row><cell>OLMo</cell><cell>1.2B</cell><cell>62.50</cell><cell>32.94</cell><cell>25.86 34.45</cell><cell>69.59</cell><cell>73.70 36.74 41.14</cell><cell>58.90</cell><cell>48.42</cell></row><row><cell>Cerebras-GPT</cell><cell>1.3B</cell><cell>38.51</cell><cell>42.70</cell><cell>26.66 26.10</cell><cell>63.67</cell><cell>66.75 30.33 42.42</cell><cell>53.59</cell><cell>43.41</cell></row><row><cell>Lamini</cell><cell>1.3B</cell><cell>38.05</cell><cell>36.43</cell><cell>28.47 26.62</cell><cell>64.62</cell><cell>67.89 33.39 43.19</cell><cell>50.59</cell><cell>43.25</cell></row><row><cell>OPT</cell><cell>1.3B</cell><cell>54.50</cell><cell>38.67</cell><cell>24.63 29.6</cell><cell>70.70</cell><cell>72.47 34.16 42.47</cell><cell>59.74</cell><cell>47.43</cell></row><row><cell>GPT-NEO</cell><cell>1.3B</cell><cell>48.49</cell><cell>39.61</cell><cell>24.82 31.31</cell><cell>65.67</cell><cell>71.05 34.06 41.81</cell><cell>57.06</cell><cell>45.98</cell></row><row><cell cols="2">Pythia-Deduped 1.4B</cell><cell>55.00</cell><cell>38.63</cell><cell>25.45 32.59</cell><cell>67.33</cell><cell>72.68 34.64 42.68</cell><cell>56.90</cell><cell>47.32</cell></row><row><cell>large-base</cell><cell>1.2B</cell><cell>62.99</cell><cell>35.90</cell><cell>24.79 34.55</cell><cell>68.49</cell><cell>75.57 35.31 41.96</cell><cell>62.03</cell><cell>49.06</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The computations were enabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre. We thank Sahal Shaji Mullappilly and Muhammad Maaz for their support in the evaluations on mobile platform and VLM training.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MobiLlama-Chat</head><p>We also developed the fine-tuned versions of MobiLlama-0.5B and MobiLlama-1.2B, namely MobiLlama-0.5B-Chat and MobiLlama-1.2B-Chat. We used the similar fine-tuning dataset and hyperparameters as AmberChat <ref type="bibr" target="#b14">(Liu et al., 2023b)</ref> to train our models. We use FastChat <ref type="bibr" target="#b35">(Zheng et al., 2024)</ref> to fine-tune our models on WizzardLM-196K as used by WizardLM <ref type="bibr" target="#b30">(Xu et al., 2023)</ref>. We trained our base models for 3 epochs on 8 A100s (80G), keeping the learning rate as 2 × 10 −5 , gradient accumulation steps as 16, and warmup ratio as 0.04.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The falcon series of open language models</title>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulaziz</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mérouane</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Étienne</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Mazzotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badreddine</forename><surname>Noune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Saleh Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><forename type="middle">L</forename><surname>Croci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Gennari Do Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15024</idno>
		<idno>arXiv:2110.06821</idno>
	</analytic>
	<monogr>
		<title level="m">Slicegpt: Compress large language models by deleting rows and columns</title>
				<editor>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2023. 2024. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Leveraging redundancy in attention with reuse transformers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Usvsn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2023. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Redpajama: An open source recipe to reproduce llama training dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Together Computer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Flashattention-2: Faster attention with better parallelism and work partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08691</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster</title>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurpreet</forename><surname>Gosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemant</forename><surname>Khachane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ribhu</forename><surname>Pathria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03208</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gptq: Accurate post-training quantization for generative pre-trained transformers</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Saleh Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13394</idno>
		<title level="m">Mme: A comprehensive evaluation benchmark for multimodal large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdonell</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.10256836</idno>
		<title level="m">Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite</title>
				<meeting><address><addrLine>Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wang</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>and Andy Zou. 2023. A framework for few-shot language model evaluation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05463</idno>
		<title level="m">Textbooks are all you need ii: phi-1.5 technical report</title>
				<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07958</idno>
		<title level="m">Truthfulqa: Measuring how models mimic human falsehoods</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Few-shot learning with multilingual language models</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2112.10668</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Visual instruction tuning</title>
				<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pangarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuguang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Iriondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note>Llm360: Towards fully transparent open-source llms</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2507" to="2521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Crows-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00133</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interpretability-aware redundancy reduction for vision transformers</title>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude Jeanne</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName><surname>Oliva</surname></persName>
		</author>
		<idno>App. 17/559</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pessoa Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">António</forename><forename type="middle">V</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Assogba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendra</forename><surname>Setiawan ; Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01826</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2023. 2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>One wide feedforward is all you need</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<title level="m">Socialiqa: Commonsense reasoning about social interactions</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lamini-lm: A diverse herd of distilled models from large-scale instructions</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Waheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2304.14402</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Muhammad Abdul-Mageed, and Alham Fikri Aji</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Smoothquant: Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">Guangxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickael</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="38087" to="38099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12244</idno>
		<title level="m">Wizardlm: Empowering large language models to follow complex instructions</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Peiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianduo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02385</idno>
		<title level="m">Tinyllama: An open-source small language model</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Opt: Open pretrained transformer language models</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Xunyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07633</idno>
		<title level="m">A survey on model compression for large language models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Qualitative Examples Reasoning: Human: What are the key benefits of practicing mindfulness meditation? MobiLlama-0.5B: Practicing mindfulness meditation can have several benefits, including: 1. Reduced stress and anxiety: Mindfulness meditation can help reduce stress and anxiety by focusing on the present moment and letting go of distractions</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved focus and concentration: Mindfulness meditation can help improve focus and concentration by focusing on the present moment and letting go of distractions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Better sleep: Mindfulness meditation can help improve sleep by reducing stress and anxiety and promoting relaxation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Better mood: Mindfulness meditation can help improve mood by reducing stress and anxiety and promoting relaxation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Knowledge: Human: What are some ways to reduce the carbon footprint of your home? MobiLlama-0.5B: Reducing the carbon footprint of your home can be achieved through several ways. Here are some tips: 1. Use energy-efficient lighting and appliances</title>
		<imprint/>
	</monogr>
	<note>Better relationships: Mindfulness meditation can help improve relationships by reducing stress and anxiety and promoting relaxation</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Install energy-efficient windows and doors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Use energy-efficient appliances and lighting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Reduce the use of paper and other non-biodegradable materials</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Use energy-efficient appliances and lighting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Reduce the use of plastic and other single-use materials</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Use energy-efficient appliances and lighting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reasoning: Human: How do diverse ecosystems contribute to the overall health of the planet? MobiLlama-0.5B: Different cultures and traditions contribute to the diversity of the world in many ways. For example, the Chinese culture is known for its emphasis on family, tradition, and hospitality. The Japanese culture is known for its emphasis on individualism, creativity, and innovation</title>
		<imprint/>
	</monogr>
	<note>Reduce the use of paper and other single-use materials. The Indian culture is known for its emphasis on tradition, devotion, and spirituality</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
